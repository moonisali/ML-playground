{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = f(x) = \\sum{(x^2 + 2 \\cdot x)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "y = torch.sum(x ** 2 + 2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx_analytic = 2 * x + 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(retain_graph=True)  # calculates gradient w.r.t. graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx_numeric = x.grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(torch.all(dy_dx_numeric == dy_dx_analytic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = W_{hy} h $$\n",
    "$$ p = softmax(y) $$\n",
    "$$ loss = -log(p) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "m = 20\n",
    "\n",
    "w = torch.randn(n, m, requires_grad=True)\n",
    "h = torch.randint(3, (20, 1), dtype=torch.float)\n",
    "y = torch.matmul(w, h)\n",
    "p = F.softmax(y, dim=0)\n",
    "\n",
    "label = torch.zeros_like(p)\n",
    "label[5] = 1.\n",
    "\n",
    "loss = -torch.sum(label * torch.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.4213, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_analytic_grad = torch.matmul((p - label) , h.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(w_analytic_grad,  w.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNNumpy:\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.w_1 = np.random.randn(hidden_size, input_size)\n",
    "        self.w_2 = np.random.randn(output_size, hidden_size)\n",
    "        \n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        h_1 = np.dot(self.w_1, x)\n",
    "        z_1 = sigmoid(h_1)\n",
    "        h_2 = np.dot(self.w_2, z_1)\n",
    "        z_2 = stable_softmax(h_2)\n",
    "        \n",
    "        self.cache['z_1'] = z_1\n",
    "        self.cache['z_2'] = z_2\n",
    "        return z_2\n",
    "    \n",
    "    def loss(self, x: torch.tensor, label: torch.tensor):\n",
    "        pred = self.forward(x)\n",
    "        return -np.sum(label * np.log(pred))\n",
    "    \n",
    "    def backward(self, x: torch.tensor, label: torch.tensor):\n",
    "        self.forward(x)\n",
    "        \n",
    "        z_1, z_2 = self.cache['z_1'], self.cache['z_2']\n",
    "\n",
    "        dh_2 = z_2 - label\n",
    "        dw_2 = np.dot(dh_2, z_1.T)\n",
    "        dh_1 = np.dot(self.w_2.T, dh_2) * (z_1 * (1 - z_1))\n",
    "        dw_1 = np.dot(dh_1, x.T)\n",
    "        return dw_1, dw_2 \n",
    "    \n",
    "    \n",
    "    def numerical_gradients(self, x: torch.tensor, label: torch.tensor, epsilon: float):\n",
    "        d_params = (np.zeros_like(self.w_1)*1., np.zeros_like(self.w_2)*1.)\n",
    "        params = (self.w_1, self.w_2)\n",
    "\n",
    "        # calculating numerical gradients for each parameter\n",
    "        for d_param, param in zip(d_params, params):\n",
    "\n",
    "            # iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "\n",
    "                # keeping the original value so we can reset it later\n",
    "                original_value = param[ix]\n",
    "\n",
    "                # estimating numeric gradients\n",
    "\n",
    "                # x + epsilon\n",
    "                param[ix] = original_value + epsilon\n",
    "                loss_plus = self.loss(x, label)\n",
    "\n",
    "                # x - epsilon\n",
    "                param[ix] = original_value - epsilon\n",
    "                loss_minus = self.loss(x, label)\n",
    "\n",
    "                # numeric_gradient = (f(x + delta) - f(x - delta)) / (2 * delta)\n",
    "                d_param[ix] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "                # resetting parameter to original value\n",
    "                param[ix] = original_value\n",
    "\n",
    "                it.iternext()\n",
    "\n",
    "        return d_params\n",
    "\n",
    "        \n",
    "    def gradient_check(self, \n",
    "                       x: torch.tensor,\n",
    "                       label: torch.tensor,\n",
    "                       epsilon: float = 1e-3,\n",
    "                       threshold: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Performs gradient checking for model parameters:\n",
    "         - computes the analytic gradients using our back-propagation implementation\n",
    "         - computes the numerical gradients using the two-sided epsilon method\n",
    "         - computes the relative difference between numerical and analytical gradients\n",
    "         - checks that the relative difference is less than threshold\n",
    "         - if the last check is failed, then raises an error\n",
    "        \"\"\"\n",
    "        \n",
    "        def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "            \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "            print()\n",
    "            numeratore = np.abs(a - b)\n",
    "            print(np.max(numeratore))\n",
    "            denominatore = np.abs(a) + np.abs(b)\n",
    "            result = numeratore / denominatore\n",
    "            result[np.isnan(result)] = 0\n",
    "            print(np.max(result))\n",
    "            return np.any(result > threshold)\n",
    "        \n",
    "        \n",
    "        params = ('w_1', 'w_2')\n",
    "\n",
    "        # calculating the gradients using backpropagation, aka analytic gradients\n",
    "        self.cache = {}\n",
    "        analytic_gradients = self.backward(x, label)\n",
    "#         analytic_gradients = (x.clone() for x in analytic_gradients)\n",
    "\n",
    "        # calculating numerical gradients\n",
    "        self.cache = {}\n",
    "        numeric_gradients = self.numerical_gradients(x, label, epsilon)\n",
    "\n",
    "        # gradient check for each parameter\n",
    "        for p_name, d_analytic, d_numeric in zip(params, analytic_gradients, numeric_gradients):\n",
    "            print(f\"\\nPerforming gradient check for parameter {p_name} \"\n",
    "                  f\"with size = {np.prod(d_analytic.shape)}.\")\n",
    "            \n",
    "            if (not d_analytic.shape == d_numeric.shape or\n",
    "                    check_relative_difference(d_analytic, d_numeric, threshold)):\n",
    "               \n",
    "                raise ValueError(f'Gradient check for {p_name} is failed.')\n",
    "\n",
    "            print(f\"Gradient check for parameter {p_name} is passed.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward pass:**\n",
    "\n",
    "$$ \n",
    "h_1 = w_1 \\cdot x \\\\\n",
    "z_1 = \\sigma(h_1)  \\\\ \n",
    "h_2 = w_2 \\cdot z_1 \\\\\n",
    "z_2 = softmax (h_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss - Cross Entropy:**\n",
    "\n",
    "$$ J = -label \\cdot \\log(z_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward pass:**\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial w_2} = \n",
    "\\frac {\\partial J} {\\partial h_2} \n",
    "\\frac {\\partial h_2} {\\partial w_2} = \n",
    "(z_2 - label) \\cdot z_1^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial w_1} = \n",
    "\\frac {\\partial J} {\\partial h_1} \n",
    "\\frac {\\partial h_1} {\\partial w_1} = \n",
    "\\frac {\\partial J} {\\partial h_1}  \\cdot x^T\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial h_1} = \n",
    "\\frac {\\partial J} {\\partial h_2} \n",
    "\\frac {\\partial h_2} {\\partial z_1}\n",
    "\\frac {\\partial z_1} {\\partial h_1}\n",
    "= \n",
    "\\big (w_1^T \\cdot (z_2 - label) \\big) z_1(1 - z_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the implementation of 2 layer simple neural network, which does the backpropogation mannually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "    \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "    numeratore = torch.abs(a - b)\n",
    "    denominatore = torch.abs(a) + torch.abs(b)\n",
    "    result = numeratore / denominatore\n",
    "    result[torch.isnan(result)] = 0\n",
    "    return bool(torch.any(result > threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.w_1 = torch.randn(hidden_size, input_size, dtype=torch.float64) * 0.01\n",
    "        self.w_2 = torch.randn(output_size, hidden_size, dtype=torch.float64) * 0.01\n",
    "        \n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        h_1 = torch.matmul(self.w_1, x)\n",
    "        z_1 = torch.sigmoid(h_1)\n",
    "        h_2 = torch.matmul(self.w_2, z_1)\n",
    "        z_2 = F.log_softmax(h_2, dim=0)\n",
    "        \n",
    "        self.cache['z_1'] = z_1\n",
    "        self.cache['z_2'] = z_2\n",
    "        return z_2\n",
    "    \n",
    "    def loss(self, x: torch.tensor, label: torch.tensor):\n",
    "        log_pred = self.forward(x)\n",
    "        return -torch.sum(label * log_pred)\n",
    "    \n",
    "    def backward(self, x: torch.tensor, label: torch.tensor):\n",
    "        self.forward(x)\n",
    "        \n",
    "        z_1, z_2 = self.cache['z_1'], self.cache['z_2']\n",
    "\n",
    "        dh_2 = torch.exp(z_2) - label\n",
    "        dw_2 = torch.matmul(dh_2, z_1.t())\n",
    "        dh_1 = torch.matmul(self.w_2.t(), dh_2) * (z_1 * (1 - z_1))\n",
    "        dw_1 = torch.matmul(dh_1, x.t())\n",
    "        return dw_1, dw_2\n",
    "    \n",
    "    def sgd_step(self, x: torch.tensor, label: torch.tensor, lr: float):\n",
    "        dw_1, dw_2 = self.backward(x, label)\n",
    "        self.w_1 -= lr * dw_1\n",
    "        self.w_2 -= lr * dw_2\n",
    "    \n",
    "    def numerical_gradients(self, x: torch.tensor, label: torch.tensor, epsilon: float):\n",
    "        d_params = (\n",
    "            torch.zeros_like(self.w_1, dtype=torch.float64),  \n",
    "            torch.zeros_like(self.w_2, dtype=torch.float64)\n",
    "        )\n",
    "        params = (self.w_1, self.w_2)\n",
    "\n",
    "        # calculating numerical gradients for each parameter\n",
    "        for d_param, param in zip(d_params, params):\n",
    "\n",
    "            # iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "\n",
    "                # keeping the original value so we can reset it later\n",
    "                original_value = param[ix].item()\n",
    "\n",
    "                # estimating numeric gradients\n",
    "\n",
    "                # x + epsilon\n",
    "                param[ix] = original_value + epsilon\n",
    "                loss_plus = self.loss(x, label)\n",
    "\n",
    "                # x - epsilon\n",
    "                param[ix] = original_value - epsilon\n",
    "                loss_minus = self.loss(x, label)\n",
    "\n",
    "                # numeric_gradient = (f(x + delta) - f(x - delta)) / (2 * delta)\n",
    "                d_param[ix] = ((loss_plus - loss_minus) / (2 * epsilon)).item()\n",
    "\n",
    "                # resetting parameter to original value\n",
    "                param[ix] = original_value\n",
    "                it.iternext()\n",
    "\n",
    "        return d_params\n",
    "    \n",
    "    def gradient_check(self, \n",
    "                       x: torch.tensor,\n",
    "                       label: torch.tensor,\n",
    "                       epsilon: float = 1e-1,\n",
    "                       threshold: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Performs gradient checking for model parameters:\n",
    "         - computes the analytic gradients using our back-propagation implementation\n",
    "         - computes the numerical gradients using the two-sided epsilon method\n",
    "         - computes the relative difference between numerical and analytical gradients\n",
    "         - checks that the relative difference is less than threshold\n",
    "         - if the last check is failed, then raises an error\n",
    "        \"\"\"\n",
    "        params = ('w_1', 'w_2')\n",
    "\n",
    "        # calculating the gradients using backpropagation, aka analytic gradients\n",
    "        self.cache = {}\n",
    "        analytic_gradients = self.backward(x, label)\n",
    "\n",
    "        # calculating numerical gradients\n",
    "        self.cache = {}\n",
    "        numeric_gradients = self.numerical_gradients(x, label, epsilon)\n",
    "\n",
    "        # gradient check for each parameter\n",
    "        for p_name, d_analytic, d_numeric in zip(params, analytic_gradients, numeric_gradients):\n",
    "            print(f\"\\nPerforming gradient check for parameter {p_name} \"\n",
    "                  f\"with size = {np.prod(d_analytic.shape)}.\")\n",
    "            \n",
    "            if (not d_analytic.shape == d_numeric.shape or\n",
    "                    check_relative_difference(d_analytic, d_numeric, threshold)):\n",
    "                            \n",
    "                raise ValueError(f'Gradient check for {p_name} is failed.')\n",
    "\n",
    "            print(f\"Gradient check for parameter {p_name} is passed.\")\n",
    "            \n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing implementation.\n",
      "\n",
      "Shapes are correct.\n",
      "\n",
      "Performing gradient check for parameter w_1 with size = 200.\n",
      "Gradient check for parameter w_1 is passed.\n",
      "\n",
      "Performing gradient check for parameter w_2 with size = 60.\n",
      "Gradient check for parameter w_2 is passed.\n"
     ]
    }
   ],
   "source": [
    "print('Testing implementation.')\n",
    "\n",
    "nn = SimpleNN(10, 20, 3)\n",
    "\n",
    "x = torch.arange(10, dtype=torch.float64).view(10, 1)\n",
    "label = torch.tensor([0, 0, 1.], dtype=torch.float64).reshape(3, 1)\n",
    "\n",
    "log_pred = nn.forward(x)\n",
    "pred = torch.exp(log_pred)\n",
    "\n",
    "assert pred.shape == label.shape == (3, 1)\n",
    "assert bool(abs(sum(pred) - 1.) < 1e-6)\n",
    "\n",
    "loss = nn.loss(x, label)\n",
    "assert torch.equal(loss, -log_pred[2, 0])\n",
    "\n",
    "dw_1, dw_2 = nn.backward(x, label)\n",
    "assert dw_1.shape == nn.w_1.shape == (20, 10)\n",
    "assert dw_2.shape == nn.w_2.shape == (3, 20)\n",
    "\n",
    "print('\\nShapes are correct.')\n",
    "\n",
    "nn.gradient_check(x, label, epsilon=1e-3, threshold=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply the network to simple data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (569, 30)\n",
      "Targets shape: (569,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data['data']\n",
    "label = data['target'] \n",
    "\n",
    "print(f'Data shape: {X.shape}')\n",
    "print(f'Targets shape: {label.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "label = one_hot_encoder.fit_transform(label.reshape(-1, 1))\n",
    "label = label.reshape(label.shape[0], label.shape[1], 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, label_train, label_test = train_test_split(X, label, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 30, 1), (426, 2, 1), (143, 30, 1), (143, 2, 1))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, label_train.shape, X_test.shape, label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "label_train = torch.from_numpy(label_train)\n",
    "label_test = torch.from_numpy(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "t() expects a 2D tensor, but self is 3D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-d5ac037a8846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-6ebb85f04e56>\u001b[0m in \u001b[0;36msgd_step\u001b[0;34m(self, x, label, lr)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mdw_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdw_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdw_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-6ebb85f04e56>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, label)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdh_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mdw_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mdh_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mz_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdw_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: t() expects a 2D tensor, but self is 3D"
     ]
    }
   ],
   "source": [
    "nn = SimpleNN(30, 50, 2)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "train_loss, test_loss = [], []\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss.append(np.mean([nn.loss(x, y).item() for x, y in zip(X_train, label_train)]))\n",
    "    test_loss.append(np.mean([nn.loss(x, y).item() for x, y in zip(X_test, label_test)]))\n",
    "    for x, y in zip(X_train, label_train):\n",
    "        nn.sgd_step(x, y, 1e-4)\n",
    "    \n",
    "    print('epoch: {}, train loss: {:.5f}, test loss: {:.5f}'.format(epoch, train_loss[-1], test_loss[-1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.w_1 = torch.randn(hidden_size, input_size, dtype=torch.float64, requires_grad=True)\n",
    "        self.w_2 = torch.randn(output_size, hidden_size, dtype=torch.float64, requires_grad=True)\n",
    "              \n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        h_1 = torch.matmul(self.w_1, x)\n",
    "        z_1 = torch.sigmoid(h_1)\n",
    "        h_2 = torch.matmul(self.w_2, z_1)\n",
    "        z_2 = F.log_softmax(h_2, dim=0)\n",
    "        \n",
    "        self.cache['z_1'] = z_1\n",
    "        self.cache['z_2'] = z_2\n",
    "        return z_2\n",
    "    \n",
    "    def loss(self, x: torch.tensor, label: torch.tensor):\n",
    "        log_pred = self.forward(x)\n",
    "        return -torch.sum(label * log_pred)\n",
    "    \n",
    "    def backward(self, x: torch.tensor, label: torch.tensor):\n",
    "        self.forward(x)\n",
    "        \n",
    "        z_1, z_2 = self.cache['z_1'], self.cache['z_2']\n",
    "\n",
    "        dh_2 = torch.exp(z_2) - label\n",
    "        dw_2 = torch.matmul(dh_2, z_1.t())\n",
    "        dh_1 = torch.matmul(self.w_2.t(), dh_2) * (z_1 * (1 - z_1))\n",
    "        dw_1 = torch.matmul(dh_1, x.t())\n",
    "        return dw_1, dw_2 \n",
    "    \n",
    "    \n",
    "    def numerical_gradients(self, x: torch.tensor, label: torch.tensor, epsilon: float):\n",
    "        d_params = (\n",
    "            torch.zeros_like(self.w_1, dtype=torch.float64),  \n",
    "            torch.zeros_like(self.w_2, dtype=torch.float64)\n",
    "        )\n",
    "        params = (self.w_1, self.w_2)\n",
    "\n",
    "        # calculating numerical gradients for each parameter\n",
    "        for d_param, param in zip(d_params, params):\n",
    "\n",
    "            # iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(param.detach(), flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "\n",
    "                # keeping the original value so we can reset it later\n",
    "                original_value = param[ix].item()\n",
    "\n",
    "                # estimating numeric gradients\n",
    "\n",
    "                # x + epsilon\n",
    "                param[ix] = original_value + epsilon\n",
    "                loss_plus = self.loss(x, label)\n",
    "\n",
    "                # x - epsilon\n",
    "                param[ix] = original_value - epsilon\n",
    "                loss_minus = self.loss(x, label)\n",
    "\n",
    "                # numeric_gradient = (f(x + delta) - f(x - delta)) / (2 * delta)\n",
    "                d_param[ix] = ((loss_plus - loss_minus) / (2 * epsilon)).item()\n",
    "\n",
    "                # resetting parameter to original value\n",
    "                param[ix] = original_value\n",
    "                it.iternext()\n",
    "\n",
    "        return d_params\n",
    "\n",
    "        \n",
    "    def gradient_check(self, \n",
    "                       x: torch.tensor,\n",
    "                       label: torch.tensor,\n",
    "                       epsilon: float = 1e-1,\n",
    "                       threshold: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Performs gradient checking for model parameters:\n",
    "         - computes the analytic gradients using our back-propagation implementation\n",
    "         - computes the numerical gradients using the two-sided epsilon method\n",
    "         - computes the relative difference between numerical and analytical gradients\n",
    "         - checks that the relative difference is less than threshold\n",
    "         - if the last check is failed, then raises an error\n",
    "        \"\"\"\n",
    "        \n",
    "        def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "            \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "            numeratore = torch.abs(a - b)\n",
    "            denominatore = torch.abs(a) + torch.abs(b)\n",
    "            result = numeratore / denominatore\n",
    "            result[torch.isnan(result)] = 0\n",
    "            return bool(torch.any(result > threshold))\n",
    "        \n",
    "        params = ('w_1', 'w_2')\n",
    "\n",
    "        # calculating the gradients using backpropagation, aka analytic gradients\n",
    "        self.cache = {}\n",
    "        analytic_gradients = self.backward(x, label)\n",
    "\n",
    "        # calculating numerical gradients\n",
    "        self.cache = {}\n",
    "        numeric_gradients = self.numerical_gradients(x, label, epsilon)\n",
    "\n",
    "        # gradient check for each parameter\n",
    "        for p_name, d_analytic, d_numeric in zip(params, analytic_gradients, numeric_gradients):\n",
    "            print(f\"\\nPerforming gradient check for parameter {p_name} \"\n",
    "                  f\"with size = {np.prod(d_analytic.shape)}.\")\n",
    "            \n",
    "            if (not d_analytic.shape == d_numeric.shape or\n",
    "                    check_relative_difference(d_analytic, d_numeric, threshold)):\n",
    "                            \n",
    "                raise ValueError(f'Gradient check for {p_name} is failed.')\n",
    "\n",
    "            print(f\"Gradient check for parameter {p_name} is passed.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing implementation.\n",
      "tensor([1.0000], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([[0.9810],\n",
      "        [0.0142],\n",
      "        [0.0048]], dtype=torch.float64, grad_fn=<ExpBackward>)\n",
      "5.333037773402289\n"
     ]
    }
   ],
   "source": [
    "print('Testing implementation.')\n",
    "\n",
    "nn = SimpleNN(10, 20, 3)\n",
    "# nn.w_1 = w_1\n",
    "# nn.w_2 = w_2\n",
    "\n",
    "x = torch.arange(10, dtype=torch.float64).view(10, 1)\n",
    "label = torch.tensor([0, 0, 1.], dtype=torch.float64).reshape(3, 1)\n",
    "\n",
    "log_pred = nn.forward(x)\n",
    "pred = torch.exp(log_pred)\n",
    "\n",
    "print(sum(pred))\n",
    "print(pred)\n",
    "assert pred.shape == label.shape == (3, 1)\n",
    "assert bool(abs(sum(pred) - 1.) < 1e-6)\n",
    "\n",
    "loss = nn.loss(x, label)\n",
    "print(loss.item())\n",
    "assert torch.equal(loss, -log_pred[2, 0])\n",
    "\n",
    "dw_1, dw_2 = nn.backward(x, label)\n",
    "assert dw_1.shape == nn.w_1.shape == (20, 10)\n",
    "assert dw_2.shape == nn.w_2.shape == (3, 20)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "# nn.gradient_check(x, label, epsilon=1e-2, threshold=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(nn.w_1.grad.data, dw_1, atol=1e-15, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(nn.w_2.grad.data, dw_2, atol=1e-15, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "    \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "    numeratore = torch.abs(a - b)\n",
    "    denominatore = torch.abs(a) + torch.abs(b)\n",
    "    result = numeratore / denominatore\n",
    "    result[torch.isnan(result)] = 0\n",
    "    return bool(torch.any(result > threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_relative_difference(nn.w_1.grad.data, dw_1, 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_relative_difference(nn.w_2.grad.data, dw_2, 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
