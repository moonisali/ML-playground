{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = f(x) = \\sum{(x^2 + 2 \\cdot x)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "y = torch.sum(x ** 2 + 2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx_analytic = 2 * x + 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(retain_graph=True)  # calculates gradient w.r.t. graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx_numeric = x.grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(torch.all(dy_dx_numeric == dy_dx_analytic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = W_{hy} h $$\n",
    "$$ p = softmax(y) $$\n",
    "$$ loss = -log(p) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "m = 20\n",
    "\n",
    "w = torch.randn(n, m, requires_grad=True)\n",
    "h = torch.randint(3, (20, 1), dtype=torch.float)\n",
    "y = torch.matmul(w, h)\n",
    "p = F.softmax(y, dim=0)\n",
    "\n",
    "label = torch.zeros_like(p)\n",
    "label[5] = 1.\n",
    "\n",
    "loss = -torch.sum(label * torch.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.7521, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_analytic_grad = torch.matmul((p - label) , h.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(w_analytic_grad,  w.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNNumpy:\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.w_1 = np.random.randn(hidden_size, input_size)\n",
    "        self.w_2 = np.random.randn(output_size, hidden_size)\n",
    "        \n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        h_1 = np.dot(self.w_1, x)\n",
    "        z_1 = sigmoid(h_1)\n",
    "        h_2 = np.dot(self.w_2, z_1)\n",
    "        z_2 = stable_softmax(h_2)\n",
    "        \n",
    "        self.cache['z_1'] = z_1\n",
    "        self.cache['z_2'] = z_2\n",
    "        return z_2\n",
    "    \n",
    "    def loss(self, x: torch.tensor, label: torch.tensor):\n",
    "        pred = self.forward(x)\n",
    "        return -np.sum(label * np.log(pred))\n",
    "    \n",
    "    def backward(self, x: torch.tensor, label: torch.tensor):\n",
    "        self.forward(x)\n",
    "        \n",
    "        z_1, z_2 = self.cache['z_1'], self.cache['z_2']\n",
    "\n",
    "        dh_2 = z_2 - label\n",
    "        dw_2 = np.dot(dh_2, z_1.T)\n",
    "        dh_1 = np.dot(self.w_2.T, dh_2) * (z_1 * (1 - z_1))\n",
    "        dw_1 = np.dot(dh_1, x.T)\n",
    "        return dw_1, dw_2 \n",
    "    \n",
    "    \n",
    "    def numerical_gradients(self, x: torch.tensor, label: torch.tensor, epsilon: float):\n",
    "        d_params = (np.zeros_like(self.w_1)*1., np.zeros_like(self.w_2)*1.)\n",
    "        params = (self.w_1, self.w_2)\n",
    "\n",
    "        # calculating numerical gradients for each parameter\n",
    "        for d_param, param in zip(d_params, params):\n",
    "\n",
    "            # iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "\n",
    "                # keeping the original value so we can reset it later\n",
    "                original_value = param[ix]\n",
    "\n",
    "                # estimating numeric gradients\n",
    "\n",
    "                # x + epsilon\n",
    "                param[ix] = original_value + epsilon\n",
    "                loss_plus = self.loss(x, label)\n",
    "\n",
    "                # x - epsilon\n",
    "                param[ix] = original_value - epsilon\n",
    "                loss_minus = self.loss(x, label)\n",
    "\n",
    "                # numeric_gradient = (f(x + delta) - f(x - delta)) / (2 * delta)\n",
    "                d_param[ix] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "                # resetting parameter to original value\n",
    "                param[ix] = original_value\n",
    "\n",
    "                it.iternext()\n",
    "\n",
    "        return d_params\n",
    "\n",
    "        \n",
    "    def gradient_check(self, \n",
    "                       x: torch.tensor,\n",
    "                       label: torch.tensor,\n",
    "                       epsilon: float = 1e-3,\n",
    "                       threshold: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Performs gradient checking for model parameters:\n",
    "         - computes the analytic gradients using our back-propagation implementation\n",
    "         - computes the numerical gradients using the two-sided epsilon method\n",
    "         - computes the relative difference between numerical and analytical gradients\n",
    "         - checks that the relative difference is less than threshold\n",
    "         - if the last check is failed, then raises an error\n",
    "        \"\"\"\n",
    "        \n",
    "        def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "            \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "            print()\n",
    "            numeratore = np.abs(a - b)\n",
    "            print(np.max(numeratore))\n",
    "            denominatore = np.abs(a) + np.abs(b)\n",
    "            result = numeratore / denominatore\n",
    "            result[np.isnan(result)] = 0\n",
    "            print(np.max(result))\n",
    "            return np.any(result > threshold)\n",
    "        \n",
    "        \n",
    "        params = ('w_1', 'w_2')\n",
    "\n",
    "        # calculating the gradients using backpropagation, aka analytic gradients\n",
    "        self.cache = {}\n",
    "        analytic_gradients = self.backward(x, label)\n",
    "#         analytic_gradients = (x.clone() for x in analytic_gradients)\n",
    "\n",
    "        # calculating numerical gradients\n",
    "        self.cache = {}\n",
    "        numeric_gradients = self.numerical_gradients(x, label, epsilon)\n",
    "\n",
    "        # gradient check for each parameter\n",
    "        for p_name, d_analytic, d_numeric in zip(params, analytic_gradients, numeric_gradients):\n",
    "            print(f\"\\nPerforming gradient check for parameter {p_name} \"\n",
    "                  f\"with size = {np.prod(d_analytic.shape)}.\")\n",
    "            \n",
    "            if (not d_analytic.shape == d_numeric.shape or\n",
    "                    check_relative_difference(d_analytic, d_numeric, threshold)):\n",
    "               \n",
    "                raise ValueError(f'Gradient check for {p_name} is failed.')\n",
    "\n",
    "            print(f\"Gradient check for parameter {p_name} is passed.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward pass:**\n",
    "\n",
    "$$ \n",
    "h_1 = w_1 \\cdot x \\\\\n",
    "z_1 = \\sigma(h_1)  \\\\ \n",
    "h_2 = w_2 \\cdot z_1 \\\\\n",
    "z_2 = softmax (h_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss - Cross Entropy:**\n",
    "\n",
    "$$ J = -label \\cdot \\log(z_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward pass:**\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial w_2} = \n",
    "\\frac {\\partial J} {\\partial h_2} \n",
    "\\frac {\\partial h_2} {\\partial w_2} = \n",
    "(z_2 - label) \\cdot z_1^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial w_1} = \n",
    "\\frac {\\partial J} {\\partial h_1} \n",
    "\\frac {\\partial h_1} {\\partial w_1} = \n",
    "\\frac {\\partial J} {\\partial h_1}  \\cdot x^T\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial h_1} = \n",
    "\\frac {\\partial J} {\\partial h_2} \n",
    "\\frac {\\partial h_2} {\\partial z_1}\n",
    "\\frac {\\partial z_1} {\\partial h_1}\n",
    "= \n",
    "\\big (w_1^T \\cdot (z_2 - label) \\big) z_1(1 - z_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the implementation of 2 layer simple neural network, which does the backpropogation mannually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "    \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "    numeratore = torch.abs(a - b)\n",
    "    denominatore = torch.abs(a) + torch.abs(b)\n",
    "    result = numeratore / denominatore\n",
    "    result[torch.isnan(result)] = 0\n",
    "    return bool(torch.any(result > threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.w_1 = torch.randn(hidden_size, input_size, dtype=torch.float64) * 0.01\n",
    "        self.w_2 = torch.randn(output_size, hidden_size, dtype=torch.float64) * 0.01\n",
    "        \n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        h_1 = torch.matmul(self.w_1, x)\n",
    "        z_1 = torch.sigmoid(h_1)\n",
    "        h_2 = torch.matmul(self.w_2, z_1)\n",
    "        z_2 = F.log_softmax(h_2, dim=0)\n",
    "        \n",
    "        self.cache['z_1'] = z_1\n",
    "        self.cache['z_2'] = z_2\n",
    "        return z_2\n",
    "    \n",
    "    def loss(self, x: torch.tensor, label: torch.tensor):\n",
    "        log_pred = self.forward(x)\n",
    "        return -torch.sum(label * log_pred)\n",
    "    \n",
    "    def backward(self, x: torch.tensor, label: torch.tensor):\n",
    "        self.forward(x)\n",
    "        \n",
    "        z_1, z_2 = self.cache['z_1'], self.cache['z_2']\n",
    "\n",
    "        dh_2 = torch.exp(z_2) - label\n",
    "        dw_2 = torch.matmul(dh_2, z_1.t())\n",
    "        dh_1 = torch.matmul(self.w_2.t(), dh_2) * (z_1 * (1 - z_1))\n",
    "        dw_1 = torch.matmul(dh_1, x.t())\n",
    "        return dw_1, dw_2 \n",
    "    \n",
    "    \n",
    "    def numerical_gradients(self, x: torch.tensor, label: torch.tensor, epsilon: float):\n",
    "        d_params = (\n",
    "            torch.zeros_like(self.w_1, dtype=torch.float64),  \n",
    "            torch.zeros_like(self.w_2, dtype=torch.float64)\n",
    "        )\n",
    "        params = (self.w_1, self.w_2)\n",
    "\n",
    "        # calculating numerical gradients for each parameter\n",
    "        for d_param, param in zip(d_params, params):\n",
    "\n",
    "            # iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "\n",
    "                # keeping the original value so we can reset it later\n",
    "                original_value = param[ix].item()\n",
    "\n",
    "                # estimating numeric gradients\n",
    "\n",
    "                # x + epsilon\n",
    "                param[ix] = original_value + epsilon\n",
    "                loss_plus = self.loss(x, label)\n",
    "\n",
    "                # x - epsilon\n",
    "                param[ix] = original_value - epsilon\n",
    "                loss_minus = self.loss(x, label)\n",
    "\n",
    "                # numeric_gradient = (f(x + delta) - f(x - delta)) / (2 * delta)\n",
    "                d_param[ix] = ((loss_plus - loss_minus) / (2 * epsilon)).item()\n",
    "\n",
    "                # resetting parameter to original value\n",
    "                param[ix] = original_value\n",
    "                it.iternext()\n",
    "\n",
    "        return d_params\n",
    "    \n",
    "    def sgd_step(self, x: torch.tensor, label: torch.tensor, lr: float):\n",
    "        pass\n",
    "\n",
    "        \n",
    "    def gradient_check(self, \n",
    "                       x: torch.tensor,\n",
    "                       label: torch.tensor,\n",
    "                       epsilon: float = 1e-1,\n",
    "                       threshold: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Performs gradient checking for model parameters:\n",
    "         - computes the analytic gradients using our back-propagation implementation\n",
    "         - computes the numerical gradients using the two-sided epsilon method\n",
    "         - computes the relative difference between numerical and analytical gradients\n",
    "         - checks that the relative difference is less than threshold\n",
    "         - if the last check is failed, then raises an error\n",
    "        \"\"\"\n",
    "        params = ('w_1', 'w_2')\n",
    "\n",
    "        # calculating the gradients using backpropagation, aka analytic gradients\n",
    "        self.cache = {}\n",
    "        analytic_gradients = self.backward(x, label)\n",
    "\n",
    "        # calculating numerical gradients\n",
    "        self.cache = {}\n",
    "        numeric_gradients = self.numerical_gradients(x, label, epsilon)\n",
    "\n",
    "        # gradient check for each parameter\n",
    "        for p_name, d_analytic, d_numeric in zip(params, analytic_gradients, numeric_gradients):\n",
    "            print(f\"\\nPerforming gradient check for parameter {p_name} \"\n",
    "                  f\"with size = {np.prod(d_analytic.shape)}.\")\n",
    "            \n",
    "            if (not d_analytic.shape == d_numeric.shape or\n",
    "                    check_relative_difference(d_analytic, d_numeric, threshold)):\n",
    "                            \n",
    "                raise ValueError(f'Gradient check for {p_name} is failed.')\n",
    "\n",
    "            print(f\"Gradient check for parameter {p_name} is passed.\")\n",
    "            \n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing implementation.\n",
      "\n",
      "Shapes are correct.\n",
      "\n",
      "Performing gradient check for parameter w_1 with size = 200.\n",
      "Gradient check for parameter w_1 is passed.\n",
      "\n",
      "Performing gradient check for parameter w_2 with size = 60.\n",
      "Gradient check for parameter w_2 is passed.\n"
     ]
    }
   ],
   "source": [
    "print('Testing implementation.')\n",
    "\n",
    "nn = SimpleNN(10, 20, 3)\n",
    "\n",
    "x = torch.arange(10, dtype=torch.float64).view(10, 1)\n",
    "label = torch.tensor([0, 0, 1.], dtype=torch.float64).reshape(3, 1)\n",
    "\n",
    "log_pred = nn.forward(x)\n",
    "pred = torch.exp(log_pred)\n",
    "\n",
    "assert pred.shape == label.shape == (3, 1)\n",
    "assert bool(abs(sum(pred) - 1.) < 1e-6)\n",
    "\n",
    "loss = nn.loss(x, label)\n",
    "assert torch.equal(loss, -log_pred[2, 0])\n",
    "\n",
    "dw_1, dw_2 = nn.backward(x, label)\n",
    "assert dw_1.shape == nn.w_1.shape == (20, 10)\n",
    "assert dw_2.shape == nn.w_2.shape == (3, 20)\n",
    "\n",
    "print('\\nShapes are correct.')\n",
    "\n",
    "nn.gradient_check(x, label, epsilon=1e-3, threshold=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.w_1 = torch.randn(hidden_size, input_size, dtype=torch.float64, requires_grad=True)\n",
    "        self.w_2 = torch.randn(output_size, hidden_size, dtype=torch.float64, requires_grad=True)\n",
    "              \n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        h_1 = torch.matmul(self.w_1, x)\n",
    "        z_1 = torch.sigmoid(h_1)\n",
    "        h_2 = torch.matmul(self.w_2, z_1)\n",
    "        z_2 = F.log_softmax(h_2, dim=0)\n",
    "        \n",
    "        self.cache['z_1'] = z_1\n",
    "        self.cache['z_2'] = z_2\n",
    "        return z_2\n",
    "    \n",
    "    def loss(self, x: torch.tensor, label: torch.tensor):\n",
    "        log_pred = self.forward(x)\n",
    "        return -torch.sum(label * log_pred)\n",
    "    \n",
    "    def backward(self, x: torch.tensor, label: torch.tensor):\n",
    "        self.forward(x)\n",
    "        \n",
    "        z_1, z_2 = self.cache['z_1'], self.cache['z_2']\n",
    "\n",
    "        dh_2 = torch.exp(z_2) - label\n",
    "        dw_2 = torch.matmul(dh_2, z_1.t())\n",
    "        dh_1 = torch.matmul(self.w_2.t(), dh_2) * (z_1 * (1 - z_1))\n",
    "        dw_1 = torch.matmul(dh_1, x.t())\n",
    "        return dw_1, dw_2 \n",
    "    \n",
    "    \n",
    "    def numerical_gradients(self, x: torch.tensor, label: torch.tensor, epsilon: float):\n",
    "        d_params = (\n",
    "            torch.zeros_like(self.w_1, dtype=torch.float64),  \n",
    "            torch.zeros_like(self.w_2, dtype=torch.float64)\n",
    "        )\n",
    "        params = (self.w_1, self.w_2)\n",
    "\n",
    "        # calculating numerical gradients for each parameter\n",
    "        for d_param, param in zip(d_params, params):\n",
    "\n",
    "            # iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(param.detach(), flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "\n",
    "                # keeping the original value so we can reset it later\n",
    "                original_value = param[ix].item()\n",
    "\n",
    "                # estimating numeric gradients\n",
    "\n",
    "                # x + epsilon\n",
    "                param[ix] = original_value + epsilon\n",
    "                loss_plus = self.loss(x, label)\n",
    "\n",
    "                # x - epsilon\n",
    "                param[ix] = original_value - epsilon\n",
    "                loss_minus = self.loss(x, label)\n",
    "\n",
    "                # numeric_gradient = (f(x + delta) - f(x - delta)) / (2 * delta)\n",
    "                d_param[ix] = ((loss_plus - loss_minus) / (2 * epsilon)).item()\n",
    "\n",
    "                # resetting parameter to original value\n",
    "                param[ix] = original_value\n",
    "                it.iternext()\n",
    "\n",
    "        return d_params\n",
    "\n",
    "        \n",
    "    def gradient_check(self, \n",
    "                       x: torch.tensor,\n",
    "                       label: torch.tensor,\n",
    "                       epsilon: float = 1e-1,\n",
    "                       threshold: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Performs gradient checking for model parameters:\n",
    "         - computes the analytic gradients using our back-propagation implementation\n",
    "         - computes the numerical gradients using the two-sided epsilon method\n",
    "         - computes the relative difference between numerical and analytical gradients\n",
    "         - checks that the relative difference is less than threshold\n",
    "         - if the last check is failed, then raises an error\n",
    "        \"\"\"\n",
    "        \n",
    "        def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "            \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "            numeratore = torch.abs(a - b)\n",
    "            denominatore = torch.abs(a) + torch.abs(b)\n",
    "            result = numeratore / denominatore\n",
    "            result[torch.isnan(result)] = 0\n",
    "            return bool(torch.any(result > threshold))\n",
    "        \n",
    "        params = ('w_1', 'w_2')\n",
    "\n",
    "        # calculating the gradients using backpropagation, aka analytic gradients\n",
    "        self.cache = {}\n",
    "        analytic_gradients = self.backward(x, label)\n",
    "\n",
    "        # calculating numerical gradients\n",
    "        self.cache = {}\n",
    "        numeric_gradients = self.numerical_gradients(x, label, epsilon)\n",
    "\n",
    "        # gradient check for each parameter\n",
    "        for p_name, d_analytic, d_numeric in zip(params, analytic_gradients, numeric_gradients):\n",
    "            print(f\"\\nPerforming gradient check for parameter {p_name} \"\n",
    "                  f\"with size = {np.prod(d_analytic.shape)}.\")\n",
    "            \n",
    "            if (not d_analytic.shape == d_numeric.shape or\n",
    "                    check_relative_difference(d_analytic, d_numeric, threshold)):\n",
    "                            \n",
    "                raise ValueError(f'Gradient check for {p_name} is failed.')\n",
    "\n",
    "            print(f\"Gradient check for parameter {p_name} is passed.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing implementation.\n",
      "tensor([1.0000], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([[0.9810],\n",
      "        [0.0142],\n",
      "        [0.0048]], dtype=torch.float64, grad_fn=<ExpBackward>)\n",
      "5.333037773402289\n"
     ]
    }
   ],
   "source": [
    "print('Testing implementation.')\n",
    "\n",
    "nn = SimpleNN(10, 20, 3)\n",
    "# nn.w_1 = w_1\n",
    "# nn.w_2 = w_2\n",
    "\n",
    "x = torch.arange(10, dtype=torch.float64).view(10, 1)\n",
    "label = torch.tensor([0, 0, 1.], dtype=torch.float64).reshape(3, 1)\n",
    "\n",
    "log_pred = nn.forward(x)\n",
    "pred = torch.exp(log_pred)\n",
    "\n",
    "print(sum(pred))\n",
    "print(pred)\n",
    "assert pred.shape == label.shape == (3, 1)\n",
    "assert bool(abs(sum(pred) - 1.) < 1e-6)\n",
    "\n",
    "loss = nn.loss(x, label)\n",
    "print(loss.item())\n",
    "assert torch.equal(loss, -log_pred[2, 0])\n",
    "\n",
    "dw_1, dw_2 = nn.backward(x, label)\n",
    "assert dw_1.shape == nn.w_1.shape == (20, 10)\n",
    "assert dw_2.shape == nn.w_2.shape == (3, 20)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "# nn.gradient_check(x, label, epsilon=1e-2, threshold=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(nn.w_1.grad.data, dw_1, atol=1e-15, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(nn.w_2.grad.data, dw_2, atol=1e-15, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "    \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "    numeratore = torch.abs(a - b)\n",
    "    denominatore = torch.abs(a) + torch.abs(b)\n",
    "    result = numeratore / denominatore\n",
    "    result[torch.isnan(result)] = 0\n",
    "    return bool(torch.any(result > threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_relative_difference(nn.w_1.grad.data, dw_1, 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_relative_difference(nn.w_2.grad.data, dw_2, 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
