{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = f(x) = \\sum{(x^2 + 2 \\cdot x)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "y = torch.sum(x ** 2 + 2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx_analytic = 2 * x + 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(retain_graph=True)  # calculates gradient w.r.t. graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx_numeric = x.grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(torch.all(dy_dx_numeric == dy_dx_analytic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = W_{hy} h $$\n",
    "$$ p = softmax(y) $$\n",
    "$$ loss = -log(p) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "m = 20\n",
    "\n",
    "w = torch.randn(n, m, requires_grad=True)\n",
    "h = torch.randint(3, (20, 1), dtype=torch.float)\n",
    "y = torch.matmul(w, h)\n",
    "p = F.softmax(y, dim=0)\n",
    "\n",
    "label = torch.zeros_like(p)\n",
    "label[5] = 1.\n",
    "\n",
    "loss = -torch.sum(label * torch.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4249, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_analytic_grad = torch.matmul((p - label) , h.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(torch.all(w_analytic_grad == w.grad.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(w_analytic_grad,  w.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.67595407e-02,  0.00000000e+00,  5.35190813e-02,\n",
       "         2.67595407e-02,  2.67595407e-02,  5.35190813e-02,\n",
       "         2.67595407e-02,  5.35190813e-02,  2.67595407e-02,\n",
       "         2.67595407e-02,  5.35190813e-02,  2.67595407e-02,\n",
       "         2.67595407e-02,  5.35190813e-02,  5.35190813e-02,\n",
       "         2.67595407e-02,  2.67595407e-02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         3.22196365e-06,  0.00000000e+00,  6.44392730e-06,\n",
       "         3.22196365e-06,  3.22196365e-06,  6.44392730e-06,\n",
       "         3.22196365e-06,  6.44392730e-06,  3.22196365e-06,\n",
       "         3.22196365e-06,  6.44392730e-06,  3.22196365e-06,\n",
       "         3.22196365e-06,  6.44392730e-06,  6.44392730e-06,\n",
       "         3.22196365e-06,  3.22196365e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.12921158e-02,  0.00000000e+00,  2.25842316e-02,\n",
       "         1.12921158e-02,  1.12921158e-02,  2.25842316e-02,\n",
       "         1.12921158e-02,  2.25842316e-02,  1.12921158e-02,\n",
       "         1.12921158e-02,  2.25842316e-02,  1.12921158e-02,\n",
       "         1.12921158e-02,  2.25842316e-02,  2.25842316e-02,\n",
       "         1.12921158e-02,  1.12921158e-02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         7.55801921e-06,  0.00000000e+00,  1.51160384e-05,\n",
       "         7.55801921e-06,  7.55801921e-06,  1.51160384e-05,\n",
       "         7.55801921e-06,  1.51160384e-05,  7.55801921e-06,\n",
       "         7.55801921e-06,  1.51160384e-05,  7.55801921e-06,\n",
       "         7.55801921e-06,  1.51160384e-05,  1.51160384e-05,\n",
       "         7.55801921e-06,  7.55801921e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         9.77020059e-03,  0.00000000e+00,  1.95404012e-02,\n",
       "         9.77020059e-03,  9.77020059e-03,  1.95404012e-02,\n",
       "         9.77020059e-03,  1.95404012e-02,  9.77020059e-03,\n",
       "         9.77020059e-03,  1.95404012e-02,  9.77020059e-03,\n",
       "         9.77020059e-03,  1.95404012e-02,  1.95404012e-02,\n",
       "         9.77020059e-03,  9.77020059e-03],\n",
       "       [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -9.99970317e-01, -0.00000000e+00, -1.99994063e+00,\n",
       "        -9.99970317e-01, -9.99970317e-01, -1.99994063e+00,\n",
       "        -9.99970317e-01, -1.99994063e+00, -9.99970317e-01,\n",
       "        -9.99970317e-01, -1.99994063e+00, -9.99970317e-01,\n",
       "        -9.99970317e-01, -1.99994063e+00, -1.99994063e+00,\n",
       "        -9.99970317e-01, -9.99970317e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         9.92350824e-09,  0.00000000e+00,  1.98470165e-08,\n",
       "         9.92350824e-09,  9.92350824e-09,  1.98470165e-08,\n",
       "         9.92350824e-09,  1.98470165e-08,  9.92350824e-09,\n",
       "         9.92350824e-09,  1.98470165e-08,  9.92350824e-09,\n",
       "         9.92350824e-09,  1.98470165e-08,  1.98470165e-08,\n",
       "         9.92350824e-09,  9.92350824e-09],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.05424215e-05,  0.00000000e+00,  2.10848430e-05,\n",
       "         1.05424215e-05,  1.05424215e-05,  2.10848430e-05,\n",
       "         1.05424215e-05,  2.10848430e-05,  1.05424215e-05,\n",
       "         1.05424215e-05,  2.10848430e-05,  1.05424215e-05,\n",
       "         1.05424215e-05,  2.10848430e-05,  2.10848430e-05,\n",
       "         1.05424215e-05,  1.05424215e-05],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         9.51947689e-01,  0.00000000e+00,  1.90389538e+00,\n",
       "         9.51947689e-01,  9.51947689e-01,  1.90389538e+00,\n",
       "         9.51947689e-01,  1.90389538e+00,  9.51947689e-01,\n",
       "         9.51947689e-01,  1.90389538e+00,  9.51947689e-01,\n",
       "         9.51947689e-01,  1.90389538e+00,  1.90389538e+00,\n",
       "         9.51947689e-01,  9.51947689e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.79388473e-04,  0.00000000e+00,  3.58776946e-04,\n",
       "         1.79388473e-04,  1.79388473e-04,  3.58776946e-04,\n",
       "         1.79388473e-04,  3.58776946e-04,  1.79388473e-04,\n",
       "         1.79388473e-04,  3.58776946e-04,  1.79388473e-04,\n",
       "         1.79388473e-04,  3.58776946e-04,  3.58776946e-04,\n",
       "         1.79388473e-04,  1.79388473e-04]], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_analytic_grad.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward pass:**\n",
    "\n",
    "$$ \n",
    "h_1 = w_1 \\cdot x \\\\\n",
    "z_1 = \\sigma(h_1)  \\\\ \n",
    "h_2 = w_2 \\cdot z_1 \\\\\n",
    "z_2 = softmax (h_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss - Cross Entropy:**\n",
    "\n",
    "$$ J = -label \\cdot \\log(z_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward pass:**\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial w_2} = \n",
    "\\frac {\\partial J} {\\partial h_2} \n",
    "\\frac {\\partial h_2} {\\partial w_2} = \n",
    "(z_2 - label) \\cdot z_1^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial w_1} = \n",
    "\\frac {\\partial J} {\\partial h_1} \n",
    "\\frac {\\partial h_1} {\\partial w_1} = \n",
    "\\frac {\\partial J} {\\partial h_1}  \\cdot x^T\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial h_1} = \n",
    "\\frac {\\partial J} {\\partial h_2} \n",
    "\\frac {\\partial h_2} {\\partial z_1}\n",
    "\\frac {\\partial z_1} {\\partial h_1}\n",
    "= \n",
    "\\big (w_1^T \\cdot (z_2 - label) \\big) z_1(1 - z_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.w_1 = torch.randn(hidden_size, input_size, dtype=torch.float)\n",
    "        self.w_2 = torch.randn(output_size, hidden_size, dtype=torch.float)\n",
    "        \n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        h_1 = torch.mm(self.w_1, x)\n",
    "        z_1 = torch.sigmoid(h_1)\n",
    "        h_2 = torch.mm(self.w_2, z_1)\n",
    "        z_2 = F.softmax(h_2, dim=0)\n",
    "        \n",
    "        self.cache['z_1'] = z_1\n",
    "        self.cache['z_2'] = z_2\n",
    "        return z_2\n",
    "    \n",
    "    def loss(self, x: torch.tensor, label: torch.tensor):\n",
    "        pred = self.forward(x)\n",
    "        return -torch.sum(label * torch.log(pred))\n",
    "    \n",
    "    def backward(self, x: torch.tensor, label: torch.tensor):\n",
    "        self.forward(x)\n",
    "        \n",
    "        z_1, z_2 = self.cache['z_1'], self.cache['z_2']\n",
    "\n",
    "        dh_2 = z_2 - label\n",
    "        dw_2 = torch.mm(dh_2, z_1.t())\n",
    "        dh_1 = torch.mm(self.w_2.t(), dh_2) * (z_1 * (1 - z_1))\n",
    "        dw_1 = torch.mm(dh_1, x.t())\n",
    "        return dw_1, dw_2 \n",
    "    \n",
    "    \n",
    "    def numerical_gradients(self, x: torch.tensor, label: torch.tensor, epsilon: float):\n",
    "        d_params = (torch.zeros_like(self.w_1, dtype=torch.float), torch.zeros_like(self.w_2, dtype=torch.float))\n",
    "        params = (self.w_1, self.w_2)\n",
    "\n",
    "        # calculating numerical gradients for each parameter\n",
    "        for d_param, param in zip(d_params, params):\n",
    "\n",
    "            # iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(param.numpy(), flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "\n",
    "                # keeping the original value so we can reset it later\n",
    "                original_value = param[ix]\n",
    "                print(original_value.item())\n",
    "\n",
    "                # estimating numeric gradients\n",
    "\n",
    "                # x + epsilon\n",
    "                param[ix] = original_value.item() + epsilon\n",
    "                print(param[ix].item())\n",
    "                \n",
    "                loss_plus = self.loss(x, label)\n",
    "\n",
    "                # x - epsilon\n",
    "                param[ix] = original_value - epsilon\n",
    "                loss_minus = self.loss(x, label)\n",
    "\n",
    "                # numeric_gradient = (f(x + delta) - f(x - delta)) / (2 * delta)\n",
    "                d_param[ix] = ((loss_plus - loss_minus) / (2 * epsilon)).item()\n",
    "\n",
    "                # resetting parameter to original value\n",
    "                param[ix] = original_value\n",
    "                print(param[ix].item())\n",
    "                print()\n",
    "\n",
    "                it.iternext()\n",
    "\n",
    "        return d_params\n",
    "\n",
    "        \n",
    "    def gradient_check(self, \n",
    "                       x: torch.tensor,\n",
    "                       label: torch.tensor,\n",
    "                       epsilon: float = 1e-1,\n",
    "                       threshold: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Performs gradient checking for model parameters:\n",
    "         - computes the analytic gradients using our back-propagation implementation\n",
    "         - computes the numerical gradients using the two-sided epsilon method\n",
    "         - computes the relative difference between numerical and analytical gradients\n",
    "         - checks that the relative difference is less than threshold\n",
    "         - if the last check is failed, then raises an error\n",
    "        \"\"\"\n",
    "        \n",
    "        def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "            \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "            a, b = a.numpy(), b.numpy()\n",
    "            return bool(np.all(np.abs(a - b) > threshold * (np.abs(a) + np.abs(b))))\n",
    "            return bool(torch.all(torch.abs(a - b) > threshold * (torch.abs(a) + torch.abs(b))))\n",
    "        \n",
    "        params = ('w_1', 'w_2')\n",
    "\n",
    "        # calculating the gradients using backpropagation, aka analytic gradients\n",
    "        self.cache = {}\n",
    "        analytic_gradients = self.backward(x, label)\n",
    "\n",
    "        # calculating numerical gradients\n",
    "        self.cache = {}\n",
    "        numeric_gradients = self.numerical_gradients(x, label, epsilon)\n",
    "\n",
    "        # gradient check for each parameter\n",
    "        for p_name, d_analytic, d_numeric in zip(params, analytic_gradients, numeric_gradients):\n",
    "            print(f\"\\nPerforming gradient check for parameter {p_name} \"\n",
    "                  f\"with size = {np.prod(d_analytic.shape)}.\")\n",
    "            \n",
    "            \n",
    "            if (not d_analytic.shape == d_numeric.shape or\n",
    "                    check_relative_difference(d_analytic, d_numeric, threshold)):\n",
    "                print(d_analytic != d_numeric)\n",
    "               \n",
    "                raise ValueError(f'Gradient check for {p_name} is failed.')\n",
    "\n",
    "            print(f\"Gradient check for parameter {p_name} is passed.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing implementation.\n",
      "tensor([1.0000])\n",
      "-0.2275388389825821\n",
      "-0.22653883695602417\n",
      "-0.2275388389825821\n",
      "\n",
      "-0.5861692428588867\n",
      "-0.58516925573349\n",
      "-0.5861692428588867\n",
      "\n",
      "-0.7420188784599304\n",
      "-0.7410188913345337\n",
      "-0.7420188784599304\n",
      "\n",
      "-0.3866904377937317\n",
      "-0.38569045066833496\n",
      "-0.3866904377937317\n",
      "\n",
      "-0.34474894404411316\n",
      "-0.34374895691871643\n",
      "-0.34474894404411316\n",
      "\n",
      "-0.6654117703437805\n",
      "-0.6644117832183838\n",
      "-0.6654117703437805\n",
      "\n",
      "-0.5662697553634644\n",
      "-0.5652697682380676\n",
      "-0.5662697553634644\n",
      "\n",
      "-0.49711188673973083\n",
      "-0.4961118996143341\n",
      "-0.49711188673973083\n",
      "\n",
      "-1.9876224994659424\n",
      "-1.9866224527359009\n",
      "-1.9876224994659424\n",
      "\n",
      "-0.31582775712013245\n",
      "-0.3148277699947357\n",
      "-0.31582775712013245\n",
      "\n",
      "2.596698760986328\n",
      "2.59769868850708\n",
      "2.596698760986328\n",
      "\n",
      "-0.3325255215167999\n",
      "-0.3315255343914032\n",
      "-0.3325255215167999\n",
      "\n",
      "0.382114440202713\n",
      "0.38311442732810974\n",
      "0.382114440202713\n",
      "\n",
      "-1.2076480388641357\n",
      "-1.2066479921340942\n",
      "-1.2076480388641357\n",
      "\n",
      "-0.3860839903354645\n",
      "-0.38508400321006775\n",
      "-0.3860839903354645\n",
      "\n",
      "-0.28397709131240845\n",
      "-0.2829771041870117\n",
      "-0.28397709131240845\n",
      "\n",
      "0.23067165911197662\n",
      "0.23167166113853455\n",
      "0.23067165911197662\n",
      "\n",
      "-0.21881163120269775\n",
      "-0.21781162917613983\n",
      "-0.21881163120269775\n",
      "\n",
      "-1.2703795433044434\n",
      "-1.2693794965744019\n",
      "-1.2703795433044434\n",
      "\n",
      "1.2235206365585327\n",
      "1.2245206832885742\n",
      "1.2235206365585327\n",
      "\n",
      "-0.8903070688247681\n",
      "-0.8893070816993713\n",
      "-0.8903070688247681\n",
      "\n",
      "1.2650283575057983\n",
      "1.2660284042358398\n",
      "1.2650283575057983\n",
      "\n",
      "-1.3435328006744385\n",
      "-1.342532753944397\n",
      "-1.3435328006744385\n",
      "\n",
      "1.6453217267990112\n",
      "1.6463217735290527\n",
      "1.6453217267990112\n",
      "\n",
      "-2.811366558074951\n",
      "-2.810366630554199\n",
      "-2.811366558074951\n",
      "\n",
      "-0.16885033249855042\n",
      "-0.1678503304719925\n",
      "-0.16885033249855042\n",
      "\n",
      "-0.9190373420715332\n",
      "-0.9180373549461365\n",
      "-0.9190373420715332\n",
      "\n",
      "-0.12461481243371964\n",
      "-0.12361481040716171\n",
      "-0.12461481243371964\n",
      "\n",
      "-1.9912595748901367\n",
      "-1.9902595281600952\n",
      "-1.9912595748901367\n",
      "\n",
      "-1.819525957107544\n",
      "-1.8185259103775024\n",
      "-1.819525957107544\n",
      "\n",
      "-0.9238406419754028\n",
      "-0.9228406548500061\n",
      "-0.9238406419754028\n",
      "\n",
      "-2.2677080631256104\n",
      "-2.2667081356048584\n",
      "-2.2677080631256104\n",
      "\n",
      "-0.6986857652664185\n",
      "-0.6976857781410217\n",
      "-0.6986857652664185\n",
      "\n",
      "-1.5299773216247559\n",
      "-1.5289772748947144\n",
      "-1.5299773216247559\n",
      "\n",
      "2.6675596237182617\n",
      "2.6685595512390137\n",
      "2.6675596237182617\n",
      "\n",
      "0.18255071341991425\n",
      "0.18355071544647217\n",
      "0.18255071341991425\n",
      "\n",
      "-1.420525312423706\n",
      "-1.4195252656936646\n",
      "-1.420525312423706\n",
      "\n",
      "-0.16595596075057983\n",
      "-0.1649559587240219\n",
      "-0.16595596075057983\n",
      "\n",
      "-0.1388830691576004\n",
      "-0.13788306713104248\n",
      "-0.1388830691576004\n",
      "\n",
      "0.18782593309879303\n",
      "0.18882593512535095\n",
      "0.18782593309879303\n",
      "\n",
      "-0.8148133754730225\n",
      "-0.8138133883476257\n",
      "-0.8148133754730225\n",
      "\n",
      "-2.0442769527435303\n",
      "-2.0432770252227783\n",
      "-2.0442769527435303\n",
      "\n",
      "-1.00331449508667\n",
      "-1.0023144483566284\n",
      "-1.00331449508667\n",
      "\n",
      "-0.0009003892191685736\n",
      "9.961077739717439e-05\n",
      "-0.0009003892773762345\n",
      "\n",
      "0.9524954557418823\n",
      "0.953495442867279\n",
      "0.9524954557418823\n",
      "\n",
      "0.4294479787349701\n",
      "0.4304479658603668\n",
      "0.4294479787349701\n",
      "\n",
      "-0.31935831904411316\n",
      "-0.31835833191871643\n",
      "-0.31935831904411316\n",
      "\n",
      "-0.12210012972354889\n",
      "-0.12110012769699097\n",
      "-0.12210012972354889\n",
      "\n",
      "1.066024661064148\n",
      "1.0670247077941895\n",
      "1.066024661064148\n",
      "\n",
      "-0.9361104369163513\n",
      "-0.9351104497909546\n",
      "-0.9361104369163513\n",
      "\n",
      "0.8326528668403625\n",
      "0.8336528539657593\n",
      "0.8326528668403625\n",
      "\n",
      "-0.8442970514297485\n",
      "-0.8432970643043518\n",
      "-0.8442970514297485\n",
      "\n",
      "-0.42347708344459534\n",
      "-0.4224770963191986\n",
      "-0.42347708344459534\n",
      "\n",
      "-1.8692349195480347\n",
      "-1.8682348728179932\n",
      "-1.8692349195480347\n",
      "\n",
      "-0.48083215951919556\n",
      "-0.47983217239379883\n",
      "-0.48083215951919556\n",
      "\n",
      "-0.3515017032623291\n",
      "-0.3505017161369324\n",
      "-0.3515017032623291\n",
      "\n",
      "0.20637527108192444\n",
      "0.20737527310848236\n",
      "0.20637527108192444\n",
      "\n",
      "0.24838772416114807\n",
      "0.249387726187706\n",
      "0.24838772416114807\n",
      "\n",
      "-1.563956618309021\n",
      "-1.5629565715789795\n",
      "-1.563956618309021\n",
      "\n",
      "-0.5799777507781982\n",
      "-0.5789777636528015\n",
      "-0.5799777507781982\n",
      "\n",
      "0.128464013338089\n",
      "0.1294640153646469\n",
      "0.128464013338089\n",
      "\n",
      "2.245497226715088\n",
      "2.24649715423584\n",
      "2.245497226715088\n",
      "\n",
      "0.7025301456451416\n",
      "0.7035301327705383\n",
      "0.7025301456451416\n",
      "\n",
      "-0.051911789923906326\n",
      "-0.0509117916226387\n",
      "-0.051911789923906326\n",
      "\n",
      "-0.7731310725212097\n",
      "-0.772131085395813\n",
      "-0.7731310725212097\n",
      "\n",
      "-0.2582016587257385\n",
      "-0.2572016716003418\n",
      "-0.2582016587257385\n",
      "\n",
      "0.8545194268226624\n",
      "0.8555194139480591\n",
      "0.8545194268226624\n",
      "\n",
      "0.16993768513202667\n",
      "0.1709376871585846\n",
      "0.16993768513202667\n",
      "\n",
      "1.5277191400527954\n",
      "1.528719186782837\n",
      "1.5277191400527954\n",
      "\n",
      "0.16847455501556396\n",
      "0.1694745570421219\n",
      "0.16847455501556396\n",
      "\n",
      "0.2651159167289734\n",
      "0.2661159038543701\n",
      "0.2651159167289734\n",
      "\n",
      "-1.2938694953918457\n",
      "-1.2928694486618042\n",
      "-1.2938694953918457\n",
      "\n",
      "0.18399189412593842\n",
      "0.18499189615249634\n",
      "0.18399189412593842\n",
      "\n",
      "0.7047936320304871\n",
      "0.7057936191558838\n",
      "0.7047936320304871\n",
      "\n",
      "-0.5576149821281433\n",
      "-0.5566149950027466\n",
      "-0.5576149821281433\n",
      "\n",
      "0.44023606181144714\n",
      "0.44123604893684387\n",
      "0.44023606181144714\n",
      "\n",
      "-1.0774145126342773\n",
      "-1.0764144659042358\n",
      "-1.0774145126342773\n",
      "\n",
      "1.237564206123352\n",
      "1.2385642528533936\n",
      "1.237564206123352\n",
      "\n",
      "1.451280117034912\n",
      "1.4522801637649536\n",
      "1.451280117034912\n",
      "\n",
      "-1.831944465637207\n",
      "-1.8309444189071655\n",
      "-1.831944465637207\n",
      "\n",
      "-0.37830913066864014\n",
      "-0.3773091435432434\n",
      "-0.37830913066864014\n",
      "\n",
      "-2.7643837928771973\n",
      "-2.7633838653564453\n",
      "-2.7643837928771973\n",
      "\n",
      "-0.3122190237045288\n",
      "-0.3112190365791321\n",
      "-0.3122190237045288\n",
      "\n",
      "1.2225403785705566\n",
      "1.2235404253005981\n",
      "1.2225403785705566\n",
      "\n",
      "0.27344849705696106\n",
      "0.2744484841823578\n",
      "0.27344849705696106\n",
      "\n",
      "0.506378710269928\n",
      "0.5073786973953247\n",
      "0.506378710269928\n",
      "\n",
      "-0.2812294661998749\n",
      "-0.28022947907447815\n",
      "-0.2812294661998749\n",
      "\n",
      "1.8688760995864868\n",
      "1.8698761463165283\n",
      "1.8688760995864868\n",
      "\n",
      "0.36883285641670227\n",
      "0.369832843542099\n",
      "0.36883285641670227\n",
      "\n",
      "-0.7250171899795532\n",
      "-0.7240172028541565\n",
      "-0.7250171899795532\n",
      "\n",
      "-0.5090953707695007\n",
      "-0.508095383644104\n",
      "-0.5090953707695007\n",
      "\n",
      "1.0731338262557983\n",
      "1.0741338729858398\n",
      "1.0731338262557983\n",
      "\n",
      "1.092761516571045\n",
      "1.0937615633010864\n",
      "1.092761516571045\n",
      "\n",
      "-0.8368643522262573\n",
      "-0.8358643651008606\n",
      "-0.8368643522262573\n",
      "\n",
      "-0.4891335070133209\n",
      "-0.4881335198879242\n",
      "-0.4891335070133209\n",
      "\n",
      "0.9853118062019348\n",
      "0.9863117933273315\n",
      "0.9853118062019348\n",
      "\n",
      "-1.0169111490249634\n",
      "-1.0159111022949219\n",
      "-1.0169111490249634\n",
      "\n",
      "0.13118229806423187\n",
      "0.1321823000907898\n",
      "0.13118229806423187\n",
      "\n",
      "-0.1009490117430687\n",
      "-0.09994900971651077\n",
      "-0.1009490117430687\n",
      "\n",
      "0.5904956459999084\n",
      "0.5914956331253052\n",
      "0.5904956459999084\n",
      "\n",
      "-0.5542072653770447\n",
      "-0.553207278251648\n",
      "-0.5542072653770447\n",
      "\n",
      "2.64802885055542\n",
      "2.649028778076172\n",
      "2.64802885055542\n",
      "\n",
      "-1.0996191501617432\n",
      "-1.0986191034317017\n",
      "-1.0996191501617432\n",
      "\n",
      "0.5073325634002686\n",
      "0.5083325505256653\n",
      "0.5073325634002686\n",
      "\n",
      "0.9412866830825806\n",
      "0.9422866702079773\n",
      "0.9412866830825806\n",
      "\n",
      "-0.365809828042984\n",
      "-0.3648098409175873\n",
      "-0.365809828042984\n",
      "\n",
      "-1.2339143753051758\n",
      "-1.2329143285751343\n",
      "-1.2339143753051758\n",
      "\n",
      "1.3669787645339966\n",
      "1.367978811264038\n",
      "1.3669787645339966\n",
      "\n",
      "0.059090785682201385\n",
      "0.06009078398346901\n",
      "0.059090785682201385\n",
      "\n",
      "-1.2391283512115479\n",
      "-1.2381283044815063\n",
      "-1.2391283512115479\n",
      "\n",
      "0.8962595462799072\n",
      "0.897259533405304\n",
      "0.8962595462799072\n",
      "\n",
      "2.523167133331299\n",
      "2.524167060852051\n",
      "2.523167133331299\n",
      "\n",
      "0.05819881707429886\n",
      "0.05919881537556648\n",
      "0.05819881707429886\n",
      "\n",
      "0.46961408853530884\n",
      "0.47061407566070557\n",
      "0.46961408853530884\n",
      "\n",
      "-1.940397024154663\n",
      "-1.9393969774246216\n",
      "-1.940397024154663\n",
      "\n",
      "-0.15659885108470917\n",
      "-0.15559884905815125\n",
      "-0.15659885108470917\n",
      "\n",
      "-0.8804526329040527\n",
      "-0.879452645778656\n",
      "-0.8804526329040527\n",
      "\n",
      "0.5434985160827637\n",
      "0.5444985032081604\n",
      "0.5434985160827637\n",
      "\n",
      "0.10729996860027313\n",
      "0.10829997062683105\n",
      "0.10729996860027313\n",
      "\n",
      "-0.45085301995277405\n",
      "-0.4498530328273773\n",
      "-0.45085301995277405\n",
      "\n",
      "-0.2479463666677475\n",
      "-0.24694636464118958\n",
      "-0.2479463666677475\n",
      "\n",
      "0.5790270566940308\n",
      "0.5800270438194275\n",
      "0.5790270566940308\n",
      "\n",
      "2.0454328060150146\n",
      "2.0464327335357666\n",
      "2.0454328060150146\n",
      "\n",
      "1.436245322227478\n",
      "1.4372453689575195\n",
      "1.436245322227478\n",
      "\n",
      "-0.620747447013855\n",
      "-0.6197474598884583\n",
      "-0.620747447013855\n",
      "\n",
      "-0.0006866800831630826\n",
      "0.0003133199061267078\n",
      "-0.0006866801413707435\n",
      "\n",
      "-1.3649221658706665\n",
      "-1.363922119140625\n",
      "-1.3649221658706665\n",
      "\n",
      "1.6730701923370361\n",
      "1.6740702390670776\n",
      "1.6730701923370361\n",
      "\n",
      "1.237768292427063\n",
      "1.2387683391571045\n",
      "1.237768292427063\n",
      "\n",
      "0.9458040595054626\n",
      "0.9468040466308594\n",
      "0.9458040595054626\n",
      "\n",
      "-0.45867684483528137\n",
      "-0.45767685770988464\n",
      "-0.45867684483528137\n",
      "\n",
      "-0.7804935574531555\n",
      "-0.7794935703277588\n",
      "-0.7804935574531555\n",
      "\n",
      "-0.4252406656742096\n",
      "-0.42424067854881287\n",
      "-0.4252406656742096\n",
      "\n",
      "-1.5268152952194214\n",
      "-1.5258152484893799\n",
      "-1.5268152952194214\n",
      "\n",
      "0.380634605884552\n",
      "0.38163459300994873\n",
      "0.380634605884552\n",
      "\n",
      "-0.13460278511047363\n",
      "-0.1336027830839157\n",
      "-0.13460278511047363\n",
      "\n",
      "-0.270184725522995\n",
      "-0.26918473839759827\n",
      "-0.270184725522995\n",
      "\n",
      "-0.7959486246109009\n",
      "-0.7949486374855042\n",
      "-0.7959486246109009\n",
      "\n",
      "-1.3302125930786133\n",
      "-1.3292125463485718\n",
      "-1.3302125930786133\n",
      "\n",
      "-0.8050406575202942\n",
      "-0.8040406703948975\n",
      "-0.8050406575202942\n",
      "\n",
      "0.8643998503684998\n",
      "0.8653998374938965\n",
      "0.8643998503684998\n",
      "\n",
      "0.11479510366916656\n",
      "0.11579510569572449\n",
      "0.11479510366916656\n",
      "\n",
      "-2.431088447570801\n",
      "-2.430088520050049\n",
      "-2.431088447570801\n",
      "\n",
      "-0.8105645179748535\n",
      "-0.8095645308494568\n",
      "-0.8105645179748535\n",
      "\n",
      "1.14029860496521\n",
      "1.1412986516952515\n",
      "1.14029860496521\n",
      "\n",
      "-0.6226010322570801\n",
      "-0.6216010451316833\n",
      "-0.6226010322570801\n",
      "\n",
      "-0.9449561834335327\n",
      "-0.943956196308136\n",
      "-0.9449561834335327\n",
      "\n",
      "-1.2289228439331055\n",
      "-1.227922797203064\n",
      "-1.2289228439331055\n",
      "\n",
      "1.832582950592041\n",
      "1.8335829973220825\n",
      "1.832582950592041\n",
      "\n",
      "0.5936067700386047\n",
      "0.5946067571640015\n",
      "0.5936067700386047\n",
      "\n",
      "0.6099470257759094\n",
      "0.6109470129013062\n",
      "0.6099470257759094\n",
      "\n",
      "0.4418184459209442\n",
      "0.44281843304634094\n",
      "0.4418184459209442\n",
      "\n",
      "0.762759268283844\n",
      "0.7637592554092407\n",
      "0.762759268283844\n",
      "\n",
      "0.9034084677696228\n",
      "0.9044084548950195\n",
      "0.9034084677696228\n",
      "\n",
      "0.15759941935539246\n",
      "0.15859942138195038\n",
      "0.15759941935539246\n",
      "\n",
      "-0.25873684883117676\n",
      "-0.25773686170578003\n",
      "-0.25873684883117676\n",
      "\n",
      "0.08035856485366821\n",
      "0.08135856688022614\n",
      "0.08035856485366821\n",
      "\n",
      "1.3590644598007202\n",
      "1.3600645065307617\n",
      "1.3590644598007202\n",
      "\n",
      "-1.5854229927062988\n",
      "-1.5844229459762573\n",
      "-1.5854229927062988\n",
      "\n",
      "0.5540738105773926\n",
      "0.5550737977027893\n",
      "0.5540738105773926\n",
      "\n",
      "0.07323332130908966\n",
      "0.07423332333564758\n",
      "0.07323332130908966\n",
      "\n",
      "-1.2874031066894531\n",
      "-1.2864030599594116\n",
      "-1.2874031066894531\n",
      "\n",
      "0.9621493220329285\n",
      "0.9631493091583252\n",
      "0.9621493220329285\n",
      "\n",
      "0.8869493007659912\n",
      "0.8879492878913879\n",
      "0.8869493007659912\n",
      "\n",
      "-1.166041374206543\n",
      "-1.1650413274765015\n",
      "-1.166041374206543\n",
      "\n",
      "-0.10262198746204376\n",
      "-0.10162198543548584\n",
      "-0.10262198746204376\n",
      "\n",
      "-0.1847974956035614\n",
      "-0.18379749357700348\n",
      "-0.1847974956035614\n",
      "\n",
      "1.2426074743270874\n",
      "1.243607521057129\n",
      "1.2426074743270874\n",
      "\n",
      "0.3296622931957245\n",
      "0.3306622803211212\n",
      "0.3296622931957245\n",
      "\n",
      "0.8643008470535278\n",
      "0.8653008341789246\n",
      "0.8643008470535278\n",
      "\n",
      "-1.3166313171386719\n",
      "-1.3156312704086304\n",
      "-1.3166313171386719\n",
      "\n",
      "0.6828455924987793\n",
      "0.683845579624176\n",
      "0.6828455924987793\n",
      "\n",
      "-0.2685820162296295\n",
      "-0.2675820291042328\n",
      "-0.2685820162296295\n",
      "\n",
      "-1.5697957277297974\n",
      "-1.5687956809997559\n",
      "-1.5697957277297974\n",
      "\n",
      "-1.480963945388794\n",
      "-1.4799638986587524\n",
      "-1.480963945388794\n",
      "\n",
      "1.007336139678955\n",
      "1.0083361864089966\n",
      "1.007336139678955\n",
      "\n",
      "-0.8162990212440491\n",
      "-0.8152990341186523\n",
      "-0.8162990212440491\n",
      "\n",
      "-0.48221099376678467\n",
      "-0.48121100664138794\n",
      "-0.48221099376678467\n",
      "\n",
      "0.0466860868036747\n",
      "0.04768608510494232\n",
      "0.0466860868036747\n",
      "\n",
      "-0.2723842263221741\n",
      "-0.27138423919677734\n",
      "-0.2723842263221741\n",
      "\n",
      "2.3186542987823486\n",
      "2.3196542263031006\n",
      "2.3186542987823486\n",
      "\n",
      "0.4392727315425873\n",
      "0.440272718667984\n",
      "0.4392727315425873\n",
      "\n",
      "0.404892235994339\n",
      "0.4058922231197357\n",
      "0.404892235994339\n",
      "\n",
      "0.2565765380859375\n",
      "0.25757652521133423\n",
      "0.2565765380859375\n",
      "\n",
      "-1.0898202657699585\n",
      "-1.088820219039917\n",
      "-1.0898202657699585\n",
      "\n",
      "-1.945985198020935\n",
      "-1.9449851512908936\n",
      "-1.945985198020935\n",
      "\n",
      "0.4815833270549774\n",
      "0.48258331418037415\n",
      "0.4815833270549774\n",
      "\n",
      "0.05162941291928291\n",
      "0.05262941122055054\n",
      "0.05162941291928291\n",
      "\n",
      "0.5348814129829407\n",
      "0.5358814001083374\n",
      "0.5348814129829407\n",
      "\n",
      "0.4934430718421936\n",
      "0.49444305896759033\n",
      "0.4934430718421936\n",
      "\n",
      "0.30173563957214355\n",
      "0.3027356266975403\n",
      "0.30173563957214355\n",
      "\n",
      "-0.6426263451576233\n",
      "-0.6416263580322266\n",
      "-0.6426263451576233\n",
      "\n",
      "-1.453163981437683\n",
      "-1.4521639347076416\n",
      "-1.453163981437683\n",
      "\n",
      "-1.316165804862976\n",
      "-1.3151657581329346\n",
      "-1.316165804862976\n",
      "\n",
      "-1.3226470947265625\n",
      "-1.321647047996521\n",
      "-1.3226470947265625\n",
      "\n",
      "0.2815863788127899\n",
      "0.28258636593818665\n",
      "0.2815863788127899\n",
      "\n",
      "-0.26844289898872375\n",
      "-0.267442911863327\n",
      "-0.26844289898872375\n",
      "\n",
      "-0.48450177907943726\n",
      "-0.4835017919540405\n",
      "-0.48450177907943726\n",
      "\n",
      "1.3393864631652832\n",
      "1.3403865098953247\n",
      "1.3393864631652832\n",
      "\n",
      "-2.151426076889038\n",
      "-2.150426149368286\n",
      "-2.151426076889038\n",
      "\n",
      "-0.9472591876983643\n",
      "-0.9462592005729675\n",
      "-0.9472591876983643\n",
      "\n",
      "-0.7221102118492126\n",
      "-0.7211102247238159\n",
      "-0.7221102118492126\n",
      "\n",
      "0.32559046149253845\n",
      "0.3265904486179352\n",
      "0.32559046149253845\n",
      "\n",
      "-1.1186349391937256\n",
      "-1.117634892463684\n",
      "-1.1186349391937256\n",
      "\n",
      "0.05204999819397926\n",
      "0.05304999649524689\n",
      "0.05204999819397926\n",
      "\n",
      "1.7275973558425903\n",
      "1.7285974025726318\n",
      "1.7275973558425903\n",
      "\n",
      "1.0206257104873657\n",
      "1.0216257572174072\n",
      "1.0206257104873657\n",
      "\n",
      "2.0387015342712402\n",
      "2.039701461791992\n",
      "2.0387015342712402\n",
      "\n",
      "0.5751295685768127\n",
      "0.5761295557022095\n",
      "0.5751295685768127\n",
      "\n",
      "0.16429553925991058\n",
      "0.1652955412864685\n",
      "0.16429553925991058\n",
      "\n",
      "0.5093050599098206\n",
      "0.5103050470352173\n",
      "0.5093050599098206\n",
      "\n",
      "-0.5418338775634766\n",
      "-0.5408338904380798\n",
      "-0.5418338775634766\n",
      "\n",
      "-1.0716140270233154\n",
      "-1.070613980293274\n",
      "-1.0716140270233154\n",
      "\n",
      "0.2743850648403168\n",
      "0.2753850519657135\n",
      "0.2743850648403168\n",
      "\n",
      "0.09779111295938492\n",
      "0.09879111498594284\n",
      "0.09779111295938492\n",
      "\n",
      "1.5302674770355225\n",
      "1.531267523765564\n",
      "1.5302674770355225\n",
      "\n",
      "-0.9752780795097351\n",
      "-0.9742780923843384\n",
      "-0.9752780795097351\n",
      "\n",
      "-1.8475861549377441\n",
      "-1.8465861082077026\n",
      "-1.8475861549377441\n",
      "\n",
      "1.7709614038467407\n",
      "1.7719614505767822\n",
      "1.7709614038467407\n",
      "\n",
      "-0.9653816223144531\n",
      "-0.9643816351890564\n",
      "-0.9653816223144531\n",
      "\n",
      "-0.259469598531723\n",
      "-0.2584696114063263\n",
      "-0.259469598531723\n",
      "\n",
      "-0.07954670488834381\n",
      "-0.07854670286178589\n",
      "-0.07954670488834381\n",
      "\n",
      "1.0261201858520508\n",
      "1.0271202325820923\n",
      "1.0261201858520508\n",
      "\n",
      "-1.3226312398910522\n",
      "-1.3216311931610107\n",
      "-1.3226312398910522\n",
      "\n",
      "0.11933977156877518\n",
      "0.1203397735953331\n",
      "0.11933977156877518\n",
      "\n",
      "1.3845428228378296\n",
      "1.385542869567871\n",
      "1.3845428228378296\n",
      "\n",
      "-0.9287432432174683\n",
      "-0.9277432560920715\n",
      "-0.9287432432174683\n",
      "\n",
      "-0.42514529824256897\n",
      "-0.42414531111717224\n",
      "-0.42514529824256897\n",
      "\n",
      "2.1190061569213867\n",
      "2.1200060844421387\n",
      "2.1190061569213867\n",
      "\n",
      "0.2802667021751404\n",
      "0.2812666893005371\n",
      "0.2802667021751404\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9828513264656067\n",
      "0.9838513135910034\n",
      "0.9828513264656067\n",
      "\n",
      "0.9583219885826111\n",
      "0.9593219757080078\n",
      "0.9583219885826111\n",
      "\n",
      "0.02476472593843937\n",
      "0.025764726102352142\n",
      "0.02476472593843937\n",
      "\n",
      "-0.16062599420547485\n",
      "-0.15962599217891693\n",
      "-0.16062599420547485\n",
      "\n",
      "-1.7757437229156494\n",
      "-1.774743676185608\n",
      "-1.7757437229156494\n",
      "\n",
      "0.13141369819641113\n",
      "0.13241370022296906\n",
      "0.13141369819641113\n",
      "\n",
      "-0.514609694480896\n",
      "-0.5136097073554993\n",
      "-0.514609694480896\n",
      "\n",
      "-0.33514273166656494\n",
      "-0.3341427445411682\n",
      "-0.33514273166656494\n",
      "\n",
      "-0.16588203608989716\n",
      "-0.16488203406333923\n",
      "-0.16588203608989716\n",
      "\n",
      "0.17887331545352936\n",
      "0.17987331748008728\n",
      "0.17887331545352936\n",
      "\n",
      "0.42456331849098206\n",
      "0.4255633056163788\n",
      "0.42456331849098206\n",
      "\n",
      "0.045173294842243195\n",
      "0.04617329314351082\n",
      "0.045173294842243195\n",
      "\n",
      "-0.7398527264595032\n",
      "-0.7388527393341064\n",
      "-0.7398527264595032\n",
      "\n",
      "-0.04572797939181328\n",
      "-0.044727981090545654\n",
      "-0.04572797939181328\n",
      "\n",
      "0.3701740801334381\n",
      "0.37117406725883484\n",
      "0.3701740801334381\n",
      "\n",
      "-0.750196635723114\n",
      "-0.7491966485977173\n",
      "-0.750196635723114\n",
      "\n",
      "0.8481701612472534\n",
      "0.8491701483726501\n",
      "0.8481701612472534\n",
      "\n",
      "-0.1593313217163086\n",
      "-0.15833131968975067\n",
      "-0.1593313217163086\n",
      "\n",
      "-0.3579886257648468\n",
      "-0.3569886386394501\n",
      "-0.3579886257648468\n",
      "\n",
      "-0.4690650403499603\n",
      "-0.4680650532245636\n",
      "-0.4690650403499603\n",
      "\n",
      "-1.7813663482666016\n",
      "-1.78036630153656\n",
      "-1.7813663482666016\n",
      "\n",
      "-0.4933928847312927\n",
      "-0.492392897605896\n",
      "-0.4933928847312927\n",
      "\n",
      "-0.8277146816253662\n",
      "-0.8267146944999695\n",
      "-0.8277146816253662\n",
      "\n",
      "0.2824227213859558\n",
      "0.28342270851135254\n",
      "0.2824227213859558\n",
      "\n",
      "-0.639893114566803\n",
      "-0.6388931274414062\n",
      "-0.639893114566803\n",
      "\n",
      "1.1573638916015625\n",
      "1.158363938331604\n",
      "1.1573638916015625\n",
      "\n",
      "0.8182334899902344\n",
      "0.8192334771156311\n",
      "0.8182334899902344\n",
      "\n",
      "-0.2824834883213043\n",
      "-0.2814835011959076\n",
      "-0.2824834883213043\n",
      "\n",
      "-0.5723426342010498\n",
      "-0.5713426470756531\n",
      "-0.5723426342010498\n",
      "\n",
      "-0.46003440022468567\n",
      "-0.45903441309928894\n",
      "-0.46003440022468567\n",
      "\n",
      "\n",
      "Performing gradient check for parameter w_1 with size = 200.\n",
      "Gradient check for parameter w_1 is passed.\n",
      "\n",
      "Performing gradient check for parameter w_2 with size = 60.\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Gradient check for w_2 is failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-293456d77378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-254-b80841584b4f>\u001b[0m in \u001b[0;36mgradient_check\u001b[0;34m(self, x, label, epsilon, threshold)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_analytic\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0md_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Gradient check for {p_name} is failed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Gradient check for parameter {p_name} is passed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Gradient check for w_2 is failed."
     ]
    }
   ],
   "source": [
    "print('Testing implementation.')\n",
    "\n",
    "nn = SimpleNN(10, 20, 3)\n",
    "\n",
    "x = torch.arange(10, dtype=torch.float).view(10, 1)\n",
    "label = torch.tensor([0, 0, 1.]).reshape(3, 1)\n",
    "\n",
    "pred = nn.forward(x)\n",
    "print(sum(pred))\n",
    "\n",
    "assert pred.shape == label.shape\n",
    "assert bool(abs(sum(pred) - 1.) < 1e-6)\n",
    "\n",
    "loss = nn.loss(x, label)\n",
    "assert torch.equal(loss, -torch.log(pred[2, 0]))\n",
    "\n",
    "dw_1, dw_2 = nn.backward(x, label)\n",
    "assert dw_1.shape == nn.w_1.shape\n",
    "assert dw_2.shape == nn.w_2.shape\n",
    "\n",
    "\n",
    "nn.gradient_check(x, label, epsilon=1e-3, threshold=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.w_1 = np.random.randn(hidden_size, input_size)\n",
    "        self.w_2 = np.random.randn(output_size, hidden_size)\n",
    "        \n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        h_1 = np.dot(self.w_1, x)\n",
    "        z_1 = 1. / (1 + np.exp(-h_1))\n",
    "        h_2 = np.dot(self.w_2, z_1)\n",
    "        z_2 = np.exp(h_2) / np.exp(h_2).sum()\n",
    "        \n",
    "        self.cache['z_1'] = z_1\n",
    "        self.cache['z_2'] = z_2\n",
    "        return z_2\n",
    "    \n",
    "    def loss(self, x: torch.tensor, label: torch.tensor):\n",
    "        pred = self.forward(x)\n",
    "        return -np.sum(label * np.log(pred))\n",
    "    \n",
    "    def backward(self, x: torch.tensor, label: torch.tensor):\n",
    "        self.forward(x)\n",
    "        \n",
    "        z_1, z_2 = self.cache['z_1'], self.cache['z_2']\n",
    "\n",
    "        dh_2 = z_2 - label\n",
    "        dw_2 = np.dot(dh_2, z_1.T)\n",
    "        dh_1 = np.dot(self.w_2.T, dh_2) * (z_1 * (1 - z_1))\n",
    "        dw_1 = np.dot(dh_1, x.T)\n",
    "        return dw_1, dw_2 \n",
    "    \n",
    "    \n",
    "    def numerical_gradients(self, x: torch.tensor, label: torch.tensor, epsilon: float):\n",
    "        d_params = (np.zeros_like(self.w_1), np.zeros_like(self.w_2))\n",
    "        params = (self.w_1, self.w_2)\n",
    "\n",
    "        # calculating numerical gradients for each parameter\n",
    "        for d_param, param in zip(d_params, params):\n",
    "\n",
    "            # iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "\n",
    "                # keeping the original value so we can reset it later\n",
    "                original_value = param[ix]\n",
    "\n",
    "                # estimating numeric gradients\n",
    "\n",
    "                # x + epsilon\n",
    "                param[ix] = original_value + epsilon\n",
    "                loss_plus = self.loss(x, label)\n",
    "\n",
    "                # x - epsilon\n",
    "                param[ix] = original_value - epsilon\n",
    "                loss_minus = self.loss(x, label)\n",
    "\n",
    "                # numeric_gradient = (f(x + delta) - f(x - delta)) / (2 * delta)\n",
    "                d_param[ix] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "                # resetting parameter to original value\n",
    "                param[ix] = original_value\n",
    "\n",
    "                it.iternext()\n",
    "\n",
    "        return d_params\n",
    "\n",
    "        \n",
    "    def gradient_check(self, \n",
    "                       x: torch.tensor,\n",
    "                       label: torch.tensor,\n",
    "                       epsilon: float = 1e-3,\n",
    "                       threshold: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Performs gradient checking for model parameters:\n",
    "         - computes the analytic gradients using our back-propagation implementation\n",
    "         - computes the numerical gradients using the two-sided epsilon method\n",
    "         - computes the relative difference between numerical and analytical gradients\n",
    "         - checks that the relative difference is less than threshold\n",
    "         - if the last check is failed, then raises an error\n",
    "        \"\"\"\n",
    "        \n",
    "        def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "            \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "            return bool(np.all(np.abs(a - b) > threshold * (np.abs(a) + np.abs(b))))\n",
    "        \n",
    "        params = ('w_1', 'w_2')\n",
    "\n",
    "        # calculating the gradients using backpropagation, aka analytic gradients\n",
    "        self.cache = {}\n",
    "        analytic_gradients = self.backward(x, label)\n",
    "#         analytic_gradients = (x.clone() for x in analytic_gradients)\n",
    "\n",
    "        # calculating numerical gradients\n",
    "        self.cache = {}\n",
    "        numeric_gradients = self.numerical_gradients(x, label, epsilon)\n",
    "\n",
    "        # gradient check for each parameter\n",
    "        for p_name, d_analytic, d_numeric in zip(params, analytic_gradients, numeric_gradients):\n",
    "            print(f\"\\nPerforming gradient check for parameter {p_name} \"\n",
    "                  f\"with size = {np.prod(d_analytic.shape)}.\")\n",
    "            \n",
    "            \n",
    "            print(d_numeric.shape == d_analytic.shape)\n",
    "            if (not d_analytic.shape == d_numeric.shape or\n",
    "                    check_relative_difference(d_analytic, d_numeric, threshold)):\n",
    "                print(d_analytic)\n",
    "                print(d_numeric)\n",
    "               \n",
    "                raise ValueError(f'Gradient check for {p_name} is failed.')\n",
    "\n",
    "            print(f\"Gradient check for parameter {p_name} is passed.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing implementation.\n",
      "[1.]\n",
      "\n",
      "Performing gradient check for parameter w_1 with size = 200.\n",
      "True\n",
      "Gradient check for parameter w_1 is passed.\n",
      "\n",
      "Performing gradient check for parameter w_2 with size = 60.\n",
      "True\n",
      "Gradient check for parameter w_2 is passed.\n"
     ]
    }
   ],
   "source": [
    "print('Testing implementation.')\n",
    "\n",
    "nn = SimpleNN(10, 20, 3)\n",
    "\n",
    "x = np.arange(10).reshape(10, 1) * 1.\n",
    "label = np.array([0, 0, 1.]).reshape(3, 1)\n",
    "\n",
    "pred = nn.forward(x)\n",
    "print(sum(pred))\n",
    "\n",
    "assert pred.shape == label.shape\n",
    "assert bool(abs(sum(pred) - 1.) < 1e-6)\n",
    "\n",
    "loss = nn.loss(x, label)\n",
    "assert np.equal(loss, -np.log(pred[2, 0]))\n",
    "\n",
    "dw_1, dw_2 = nn.backward(x, label)\n",
    "assert dw_1.shape == nn.w_1.shape\n",
    "assert dw_2.shape == nn.w_2.shape\n",
    "\n",
    "\n",
    "nn.gradient_check(x, label, epsilon=1e-5, threshold=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7634e-04],\n",
       "        [2.8016e-02],\n",
       "        [9.7181e-01]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
