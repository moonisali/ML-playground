{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from dataset import get_mnist_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = f(x) = \\sum{(x^2 + 2 \\cdot x)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "y = torch.sum(x ** 2 + 2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx_analytic = 2 * x + 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(retain_graph=True)  # calculates gradient w.r.t. graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx_numeric = x.grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(torch.all(dy_dx_numeric == dy_dx_analytic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = W_{hy} h $$\n",
    "$$ p = softmax(y) $$\n",
    "$$ loss = -log(p) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "m = 20\n",
    "\n",
    "w = torch.randn(n, m, requires_grad=True)\n",
    "h = torch.randint(3, (20, 1), dtype=torch.float)\n",
    "y = torch.matmul(w, h)\n",
    "p = F.softmax(y, dim=0)\n",
    "\n",
    "label = torch.zeros_like(p)\n",
    "label[5] = 1.\n",
    "\n",
    "loss = -torch.sum(label * torch.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.7621, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_analytic_grad = torch.matmul((p - label) , h.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(w_analytic_grad,  w.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N, K, H, O - batch_size, input_size, hidden_size, output_size\n",
    "\n",
    "\n",
    "* $x$: input, shape: (N, K)\n",
    "\n",
    "* $w_1$: hidden layer weights, shape: (K, H)\n",
    "\n",
    "* $w_2$: output layer weights, shape: (H, O)\n",
    "\n",
    "* $z_2$: output, shape (N, O)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward pass:**\n",
    "\n",
    "$$ \n",
    "h_1 = x \\cdot w_{1}\\\\\n",
    "z_1 = \\sigma(h_1)  \\\\ \n",
    "h_2 =  z_1 \\cdot w_{2} \\\\\n",
    "z_2 = softmax (h_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss - Cross Entropy:**\n",
    "\n",
    "$$ J = -label \\cdot \\log(z_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward pass:**\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial w_2} = \n",
    "\\frac {\\partial J} {\\partial h_2} \n",
    "\\frac {\\partial h_2} {\\partial w_2} = \n",
    "z_1^T \\cdot  (z_2 - label)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial w_1} = \n",
    "\\frac {\\partial J} {\\partial h_1} \n",
    "\\frac {\\partial h_1} {\\partial w_1} = \n",
    "x^T \\cdot \\frac {\\partial J} {\\partial h_1} \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J} {\\partial h_1} = \n",
    "\\frac {\\partial J} {\\partial h_2} \n",
    "\\frac {\\partial h_2} {\\partial z_1}\n",
    "\\frac {\\partial z_1} {\\partial h_1}\n",
    "= \n",
    "\\big ((z_2 - label) \\cdot w_1^T \\big) z_1(1 - z_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the implementation of 2 layer simple neural network, which does the backpropogation mannually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relative_difference(a: torch.tensor, b: torch.tensor, threshold: float) -> bool:\n",
    "    \"\"\"Returns True if (|a - b| / (|a| + |b|)) > threshold else False.\"\"\"\n",
    "    numeratore = torch.abs(a - b)\n",
    "    denominatore = torch.abs(a) + torch.abs(b)\n",
    "    result = numeratore / denominatore\n",
    "    result[torch.isnan(result)] = 0\n",
    "    return bool(torch.any(result > threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Simple Neural Network with one hidden layer for classification.\n",
    "    The backpropagation is implemented manually.\n",
    "\n",
    "    It uses sigmoid as an activation function for hidden layer and log_softmax for output layer.\n",
    "    Loss function is a cross entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, dtype: torch.dtype):\n",
    "\n",
    "        self.w_1 = torch.randn(input_size, hidden_size, dtype=dtype) * 0.01\n",
    "        self.w_2 = torch.randn(hidden_size, output_size, dtype=dtype) * 0.01\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass function.\n",
    "\n",
    "        x shape: (batch_size, input_size)\n",
    "        Returns log prediction.\n",
    "        \"\"\"\n",
    "\n",
    "        h_1 = torch.matmul(x, self.w_1)\n",
    "        z_1 = torch.sigmoid(h_1)\n",
    "\n",
    "        h_2 = torch.matmul(z_1, self.w_2)\n",
    "        z_2 = F.log_softmax(h_2, dim=1)\n",
    "\n",
    "        self.cache['z_1'] = z_1\n",
    "        self.cache['z_2'] = z_2\n",
    "        return z_2\n",
    "\n",
    "    def loss(self, x: torch.Tensor, label: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Cross entropy loss function.\n",
    "\n",
    "        x shape: (batch_size, input_size)\n",
    "        label shape: (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        log_prediction = self.forward(x)\n",
    "        return -torch.sum(label * log_prediction)\n",
    "\n",
    "    def backward(self, x: torch.Tensor, label: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Performs backpropagation, aka calculates loss gradient w.r.t. network weights.\n",
    "\n",
    "        x shape: (batch_size, input_size)\n",
    "        label shape: (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        self.forward(x)\n",
    "\n",
    "        z_1, z_2 = self.cache['z_1'], self.cache['z_2']\n",
    "\n",
    "        dh_2 = torch.exp(z_2) - label\n",
    "        dw_2 = torch.matmul(z_1.t(), dh_2)\n",
    "        dh_1 = torch.matmul(dh_2, self.w_2.t()) * (z_1 * (1 - z_1))\n",
    "        dw_1 = torch.matmul(x.t(), dh_1)\n",
    "        return dw_1, dw_2\n",
    "\n",
    "    def sgd_step(self, x: torch.Tensor, label: torch.Tensor, lr: float):\n",
    "        \"\"\"Performs simple stochastic gradient descent step.\"\"\"\n",
    "        dw_1, dw_2 = self.backward(x, label)\n",
    "        self.w_1 -= lr * dw_1\n",
    "        self.w_2 -= lr * dw_2\n",
    "\n",
    "    def numerical_gradients(self, x: torch.Tensor, label: torch.Tensor, epsilon: float):\n",
    "        \"\"\"Numerically calculates gradients.\"\"\"\n",
    "        d_params = (\n",
    "            torch.zeros_like(self.w_1, dtype=self.dtype),\n",
    "            torch.zeros_like(self.w_2, dtype=self.dtype)\n",
    "        )\n",
    "        params = (self.w_1, self.w_2)\n",
    "\n",
    "        # calculating numerical gradients for each parameter\n",
    "        for d_param, param in zip(d_params, params):\n",
    "\n",
    "            # iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "\n",
    "                # keeping the original value so we can reset it later\n",
    "                original_value = param[ix].item()\n",
    "\n",
    "                # estimating numeric gradients\n",
    "\n",
    "                # x + epsilon\n",
    "                param[ix] = original_value + epsilon\n",
    "                loss_plus = self.loss(x, label)\n",
    "\n",
    "                # x - epsilon\n",
    "                param[ix] = original_value - epsilon\n",
    "                loss_minus = self.loss(x, label)\n",
    "\n",
    "                # numeric_gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n",
    "                d_param[ix] = ((loss_plus - loss_minus) / (2 * epsilon)).item()\n",
    "\n",
    "                # resetting parameter to original value\n",
    "                param[ix] = original_value\n",
    "                it.iternext()\n",
    "\n",
    "        return d_params\n",
    "\n",
    "    def gradient_check(self,\n",
    "                       x: torch.Tensor,\n",
    "                       label: torch.Tensor,\n",
    "                       epsilon: float = 1e-1,\n",
    "                       threshold: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Performs gradient checking for model parameters:\n",
    "         - computes the analytic gradients using our back-propagation implementation\n",
    "         - computes the numerical gradients using the two-sided epsilon method\n",
    "         - computes the relative difference between numerical and analytical gradients\n",
    "         - checks that the relative difference is less than threshold\n",
    "         - if the last check is failed, then raises an error\n",
    "        \"\"\"\n",
    "        params = ('w_1', 'w_2')\n",
    "\n",
    "        # calculating the gradients using backpropagation, aka analytic gradients\n",
    "        self.cache = {}\n",
    "        analytic_gradients = self.backward(x, label)\n",
    "\n",
    "        # calculating numerical gradients\n",
    "        self.cache = {}\n",
    "        numeric_gradients = self.numerical_gradients(x, label, epsilon)\n",
    "\n",
    "        # gradient check for each parameter\n",
    "        for p_name, d_analytic, d_numeric in zip(params, analytic_gradients, numeric_gradients):\n",
    "            print(f\"\\nPerforming gradient check for parameter {p_name} \"\n",
    "                  f\"with size = {np.prod(d_analytic.shape)}.\")\n",
    "\n",
    "            if (not d_analytic.shape == d_numeric.shape or\n",
    "                    check_relative_difference(d_analytic, d_numeric, threshold)):\n",
    "                raise ValueError(f'Gradient check for {p_name} is failed.')\n",
    "\n",
    "            print(f\"Gradient check for parameter {p_name} is passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing implementation.\n",
      "\n",
      "Shapes are correct.\n",
      "\n",
      "Performing gradient check for parameter w_1 with size = 200.\n",
      "Gradient check for parameter w_1 is passed.\n",
      "\n",
      "Performing gradient check for parameter w_2 with size = 60.\n",
      "Gradient check for parameter w_2 is passed.\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-4\n",
    "\n",
    "print('Testing implementation.')\n",
    "\n",
    "batch_size, input_size, hidden_size, output_size = 64, 784, 20, 10\n",
    "data, _ = get_mnist_data(batch_size=batch_size)\n",
    "x, label = next(data)\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size, x.dtype)\n",
    "\n",
    "log_pred = model.forward(x)\n",
    "pred = torch.exp(log_pred)\n",
    "\n",
    "assert pred.shape == label.shape == (batch_size, output_size)\n",
    "assert abs(torch.sum(pred[0]).item() - 1.) < threshold\n",
    "\n",
    "diff = abs(torch.sum(pred).item() - batch_size)\n",
    "try:\n",
    "    assert diff < threshold\n",
    "except AssertionError:\n",
    "    print(diff)\n",
    "\n",
    "loss = model.loss(x, label)\n",
    "_, indexes = np.where(label > 0.)\n",
    "diff = abs(loss.item() + log_pred[torch.arange(batch_size), indexes].sum().item())\n",
    "try:\n",
    "    assert diff < threshold\n",
    "except AssertionError:\n",
    "    print(diff)\n",
    "\n",
    "dw_1, dw_2 = model.backward(x, label)\n",
    "assert dw_1.shape == model.w_1.shape == (input_size, hidden_size)\n",
    "assert dw_2.shape == model.w_2.shape == (hidden_size, output_size)\n",
    "\n",
    "print('\\nShapes are correct.')\n",
    "dtype = torch.float64\n",
    "\n",
    "x = torch.arange(10, dtype=dtype).view(1, 10)\n",
    "label = torch.tensor([0, 0, 1.], dtype=dtype).reshape(1, 3)\n",
    "model = NeuralNetwork(10, 20, 3, dtype)\n",
    "model.gradient_check(x, label, epsilon=1e-3, threshold=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply the network to simple data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (569, 30)\n",
      "Targets shape: (569,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data['data']\n",
    "label = data['target'] \n",
    "\n",
    "print(f'Data shape: {X.shape}')\n",
    "print(f'Targets shape: {label.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "label = one_hot_encoder.fit_transform(label.reshape(-1, 1))\n",
    "label = label.reshape(label.shape[0], label.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, label_train, label_test = train_test_split(X, label, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 30), (426, 2), (143, 30), (143, 2))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, label_train.shape, X_test.shape, label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "label_train = torch.from_numpy(label_train)\n",
    "label_test = torch.from_numpy(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 299.11671, test loss: 100.44311\n",
      "epoch: 1, train loss: 295.02428, test loss: 99.07286\n",
      "epoch: 2, train loss: 292.12758, test loss: 98.13322\n",
      "epoch: 3, train loss: 290.39829, test loss: 97.56727\n",
      "epoch: 4, train loss: 289.05726, test loss: 97.11508\n",
      "epoch: 5, train loss: 287.88123, test loss: 96.74006\n",
      "epoch: 6, train loss: 286.80731, test loss: 96.36627\n",
      "epoch: 7, train loss: 285.82486, test loss: 96.06173\n",
      "epoch: 8, train loss: 284.92359, test loss: 95.72295\n",
      "epoch: 9, train loss: 284.24779, test loss: 95.51300\n",
      "epoch: 10, train loss: 283.44286, test loss: 95.24892\n",
      "epoch: 11, train loss: 282.62906, test loss: 95.00211\n",
      "epoch: 12, train loss: 282.47804, test loss: 94.90637\n",
      "epoch: 13, train loss: 281.90954, test loss: 94.73262\n",
      "epoch: 14, train loss: 281.23807, test loss: 94.54917\n",
      "epoch: 15, train loss: 280.11915, test loss: 94.14836\n",
      "epoch: 16, train loss: 280.07979, test loss: 94.18333\n",
      "epoch: 17, train loss: 279.93053, test loss: 94.05089\n",
      "epoch: 18, train loss: 280.10840, test loss: 94.16247\n",
      "epoch: 19, train loss: 277.99728, test loss: 93.50574\n",
      "epoch: 20, train loss: 278.09954, test loss: 93.50074\n",
      "epoch: 21, train loss: 277.68863, test loss: 93.43687\n",
      "epoch: 22, train loss: 279.10627, test loss: 93.77384\n",
      "epoch: 23, train loss: 276.94027, test loss: 93.16421\n",
      "epoch: 24, train loss: 276.32763, test loss: 92.91110\n",
      "epoch: 25, train loss: 276.36987, test loss: 92.99943\n",
      "epoch: 26, train loss: 275.60459, test loss: 92.67095\n",
      "epoch: 27, train loss: 276.49528, test loss: 93.00650\n",
      "epoch: 28, train loss: 275.72345, test loss: 92.79622\n",
      "epoch: 29, train loss: 274.28602, test loss: 92.33438\n",
      "epoch: 30, train loss: 273.80902, test loss: 92.24559\n",
      "epoch: 31, train loss: 275.51993, test loss: 92.67169\n",
      "epoch: 32, train loss: 274.31039, test loss: 92.36021\n",
      "epoch: 33, train loss: 274.86560, test loss: 92.49252\n",
      "epoch: 34, train loss: 274.71518, test loss: 92.46706\n",
      "epoch: 35, train loss: 272.21465, test loss: 91.67551\n",
      "epoch: 36, train loss: 274.45280, test loss: 92.40593\n",
      "epoch: 37, train loss: 272.12721, test loss: 91.65632\n",
      "epoch: 38, train loss: 273.33093, test loss: 92.07006\n",
      "epoch: 39, train loss: 271.04011, test loss: 91.39854\n",
      "epoch: 40, train loss: 270.48819, test loss: 91.14305\n",
      "epoch: 41, train loss: 271.65385, test loss: 91.72408\n",
      "epoch: 42, train loss: 273.24545, test loss: 91.90525\n",
      "epoch: 43, train loss: 270.30392, test loss: 91.36167\n",
      "epoch: 44, train loss: 274.04522, test loss: 92.18940\n",
      "epoch: 45, train loss: 270.79664, test loss: 91.30284\n",
      "epoch: 46, train loss: 268.50227, test loss: 90.63504\n",
      "epoch: 47, train loss: 269.74764, test loss: 90.97255\n",
      "epoch: 48, train loss: 271.77037, test loss: 91.62749\n",
      "epoch: 49, train loss: 267.44407, test loss: 90.27748\n",
      "epoch: 50, train loss: 266.98639, test loss: 90.25982\n",
      "epoch: 51, train loss: 267.22214, test loss: 90.25904\n",
      "epoch: 52, train loss: 271.00249, test loss: 91.38419\n",
      "epoch: 53, train loss: 266.11301, test loss: 89.92804\n",
      "epoch: 54, train loss: 269.48920, test loss: 91.06970\n",
      "epoch: 55, train loss: 269.70644, test loss: 90.92990\n",
      "epoch: 56, train loss: 266.04763, test loss: 90.10035\n",
      "epoch: 57, train loss: 267.90238, test loss: 90.45116\n",
      "epoch: 58, train loss: 267.83997, test loss: 90.49173\n",
      "epoch: 59, train loss: 266.23686, test loss: 89.87604\n",
      "epoch: 60, train loss: 267.82719, test loss: 90.67988\n",
      "epoch: 61, train loss: 267.30125, test loss: 90.19906\n",
      "epoch: 62, train loss: 266.00160, test loss: 90.01725\n",
      "epoch: 63, train loss: 264.85384, test loss: 89.54714\n",
      "epoch: 64, train loss: 268.89850, test loss: 90.89109\n",
      "epoch: 65, train loss: 263.95653, test loss: 89.13333\n",
      "epoch: 66, train loss: 265.94774, test loss: 90.14961\n",
      "epoch: 67, train loss: 266.51581, test loss: 90.29005\n",
      "epoch: 68, train loss: 267.54604, test loss: 90.18478\n",
      "epoch: 69, train loss: 265.17336, test loss: 89.87576\n",
      "epoch: 70, train loss: 260.73608, test loss: 88.14337\n",
      "epoch: 71, train loss: 262.09074, test loss: 89.12096\n",
      "epoch: 72, train loss: 263.58247, test loss: 89.02767\n",
      "epoch: 73, train loss: 263.29567, test loss: 89.14984\n",
      "epoch: 74, train loss: 262.97437, test loss: 89.15838\n",
      "epoch: 75, train loss: 260.89503, test loss: 88.25616\n",
      "epoch: 76, train loss: 261.23051, test loss: 88.75368\n",
      "epoch: 77, train loss: 263.73877, test loss: 89.18155\n",
      "epoch: 78, train loss: 259.94083, test loss: 88.42992\n",
      "epoch: 79, train loss: 261.43661, test loss: 88.28164\n",
      "epoch: 80, train loss: 263.85216, test loss: 89.65299\n",
      "epoch: 81, train loss: 256.14966, test loss: 86.77796\n",
      "epoch: 82, train loss: 261.13136, test loss: 89.17083\n",
      "epoch: 83, train loss: 267.12694, test loss: 90.21872\n",
      "epoch: 84, train loss: 260.44996, test loss: 88.20310\n",
      "epoch: 85, train loss: 260.38922, test loss: 88.66321\n",
      "epoch: 86, train loss: 256.84807, test loss: 87.00698\n",
      "epoch: 87, train loss: 254.75140, test loss: 86.96122\n",
      "epoch: 88, train loss: 259.65742, test loss: 88.35637\n",
      "epoch: 89, train loss: 265.94103, test loss: 89.87185\n",
      "epoch: 90, train loss: 258.04110, test loss: 87.53988\n",
      "epoch: 91, train loss: 255.42068, test loss: 87.29698\n",
      "epoch: 92, train loss: 263.20693, test loss: 89.04587\n",
      "epoch: 93, train loss: 253.45448, test loss: 86.43045\n",
      "epoch: 94, train loss: 256.91285, test loss: 87.03605\n",
      "epoch: 95, train loss: 254.03362, test loss: 86.68535\n",
      "epoch: 96, train loss: 255.67461, test loss: 86.93976\n",
      "epoch: 97, train loss: 256.73925, test loss: 87.50888\n",
      "epoch: 98, train loss: 256.06719, test loss: 87.12815\n",
      "epoch: 99, train loss: 262.02914, test loss: 89.09132\n",
      "epoch: 100, train loss: 254.94931, test loss: 86.56698\n",
      "epoch: 101, train loss: 257.47205, test loss: 87.47617\n",
      "epoch: 102, train loss: 252.26118, test loss: 86.45195\n",
      "epoch: 103, train loss: 254.54261, test loss: 86.54311\n",
      "epoch: 104, train loss: 256.14062, test loss: 87.56178\n",
      "epoch: 105, train loss: 252.16535, test loss: 85.61512\n",
      "epoch: 106, train loss: 248.03844, test loss: 84.94296\n",
      "epoch: 107, train loss: 247.82719, test loss: 84.96752\n",
      "epoch: 108, train loss: 254.94613, test loss: 86.67814\n",
      "epoch: 109, train loss: 249.92497, test loss: 85.42215\n",
      "epoch: 110, train loss: 249.73690, test loss: 85.72202\n",
      "epoch: 111, train loss: 255.39371, test loss: 86.72938\n",
      "epoch: 112, train loss: 247.31584, test loss: 84.60852\n",
      "epoch: 113, train loss: 248.81877, test loss: 85.08479\n",
      "epoch: 114, train loss: 251.56553, test loss: 86.02388\n",
      "epoch: 115, train loss: 251.15808, test loss: 85.30259\n",
      "epoch: 116, train loss: 247.54544, test loss: 85.14897\n",
      "epoch: 117, train loss: 252.46354, test loss: 85.96903\n",
      "epoch: 118, train loss: 249.17002, test loss: 85.03320\n",
      "epoch: 119, train loss: 240.32964, test loss: 82.37645\n",
      "epoch: 120, train loss: 242.56292, test loss: 83.60658\n",
      "epoch: 121, train loss: 245.91913, test loss: 84.02898\n",
      "epoch: 122, train loss: 245.21848, test loss: 84.08793\n",
      "epoch: 123, train loss: 236.42204, test loss: 80.91511\n",
      "epoch: 124, train loss: 246.96167, test loss: 85.15937\n",
      "epoch: 125, train loss: 243.49514, test loss: 82.96317\n",
      "epoch: 126, train loss: 240.18529, test loss: 83.05081\n",
      "epoch: 127, train loss: 241.06766, test loss: 82.55778\n",
      "epoch: 128, train loss: 249.10719, test loss: 85.68403\n",
      "epoch: 129, train loss: 241.49053, test loss: 83.22435\n",
      "epoch: 130, train loss: 243.85154, test loss: 83.17552\n",
      "epoch: 131, train loss: 250.89091, test loss: 86.18125\n",
      "epoch: 132, train loss: 236.25689, test loss: 81.28669\n",
      "epoch: 133, train loss: 241.87287, test loss: 83.06863\n",
      "epoch: 134, train loss: 245.96014, test loss: 84.21589\n",
      "epoch: 135, train loss: 239.07398, test loss: 81.82916\n",
      "epoch: 136, train loss: 226.37769, test loss: 79.58000\n",
      "epoch: 137, train loss: 241.89042, test loss: 82.14430\n",
      "epoch: 138, train loss: 253.00341, test loss: 87.24721\n",
      "epoch: 139, train loss: 237.56730, test loss: 82.82712\n",
      "epoch: 140, train loss: 252.08611, test loss: 85.57751\n",
      "epoch: 141, train loss: 244.61494, test loss: 83.77635\n",
      "epoch: 142, train loss: 234.46189, test loss: 82.21428\n",
      "epoch: 143, train loss: 243.17519, test loss: 82.48212\n",
      "epoch: 144, train loss: 242.11075, test loss: 83.84088\n",
      "epoch: 145, train loss: 237.57456, test loss: 82.09493\n",
      "epoch: 146, train loss: 241.66461, test loss: 83.30359\n",
      "epoch: 147, train loss: 231.83072, test loss: 79.89883\n",
      "epoch: 148, train loss: 229.62016, test loss: 80.08520\n",
      "epoch: 149, train loss: 228.50673, test loss: 78.83291\n",
      "epoch: 150, train loss: 234.18381, test loss: 80.96251\n",
      "epoch: 151, train loss: 225.49145, test loss: 78.37378\n",
      "epoch: 152, train loss: 229.99217, test loss: 79.95199\n",
      "epoch: 153, train loss: 234.31298, test loss: 81.01521\n",
      "epoch: 154, train loss: 233.17341, test loss: 81.03028\n",
      "epoch: 155, train loss: 231.48216, test loss: 79.57924\n",
      "epoch: 156, train loss: 238.89690, test loss: 82.93485\n",
      "epoch: 157, train loss: 227.20065, test loss: 78.68383\n",
      "epoch: 158, train loss: 231.18733, test loss: 79.95160\n",
      "epoch: 159, train loss: 219.21611, test loss: 76.89344\n",
      "epoch: 160, train loss: 230.20040, test loss: 80.52507\n",
      "epoch: 161, train loss: 228.80462, test loss: 79.21779\n",
      "epoch: 162, train loss: 226.41712, test loss: 78.62289\n",
      "epoch: 163, train loss: 221.95990, test loss: 78.32533\n",
      "epoch: 164, train loss: 234.67330, test loss: 80.92905\n",
      "epoch: 165, train loss: 229.05148, test loss: 80.42081\n",
      "epoch: 166, train loss: 225.05993, test loss: 78.74472\n",
      "epoch: 167, train loss: 221.81273, test loss: 76.63837\n",
      "epoch: 168, train loss: 221.02680, test loss: 78.14915\n",
      "epoch: 169, train loss: 221.25451, test loss: 76.68781\n",
      "epoch: 170, train loss: 235.40965, test loss: 82.77584\n",
      "epoch: 171, train loss: 220.87171, test loss: 76.66994\n",
      "epoch: 172, train loss: 211.63449, test loss: 73.96196\n",
      "epoch: 173, train loss: 214.01056, test loss: 76.57572\n",
      "epoch: 174, train loss: 225.97565, test loss: 78.10869\n",
      "epoch: 175, train loss: 207.92224, test loss: 73.63009\n",
      "epoch: 176, train loss: 209.92806, test loss: 73.19624\n",
      "epoch: 177, train loss: 220.25624, test loss: 78.63638\n",
      "epoch: 178, train loss: 227.75323, test loss: 78.16452\n",
      "epoch: 179, train loss: 216.86529, test loss: 77.75790\n",
      "epoch: 180, train loss: 220.28954, test loss: 76.16115\n",
      "epoch: 181, train loss: 220.48661, test loss: 78.35417\n",
      "epoch: 182, train loss: 213.62300, test loss: 74.60894\n",
      "epoch: 183, train loss: 209.64213, test loss: 73.23714\n",
      "epoch: 184, train loss: 208.87457, test loss: 74.47299\n",
      "epoch: 185, train loss: 220.01301, test loss: 77.14528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 186, train loss: 210.92705, test loss: 73.09375\n",
      "epoch: 187, train loss: 226.73112, test loss: 83.61175\n",
      "epoch: 188, train loss: 264.67274, test loss: 89.40726\n",
      "epoch: 189, train loss: 219.77296, test loss: 77.05337\n",
      "epoch: 190, train loss: 207.58955, test loss: 73.10495\n",
      "epoch: 191, train loss: 204.67041, test loss: 72.63364\n",
      "epoch: 192, train loss: 205.66408, test loss: 72.40909\n",
      "epoch: 193, train loss: 209.66482, test loss: 75.00938\n",
      "epoch: 194, train loss: 208.86509, test loss: 72.57415\n",
      "epoch: 195, train loss: 203.29318, test loss: 73.09055\n",
      "epoch: 196, train loss: 205.20915, test loss: 71.65828\n",
      "epoch: 197, train loss: 210.79451, test loss: 76.55531\n",
      "epoch: 198, train loss: 212.33250, test loss: 73.87820\n",
      "epoch: 199, train loss: 209.27862, test loss: 74.51210\n",
      "epoch: 200, train loss: 198.10413, test loss: 69.61563\n",
      "epoch: 201, train loss: 204.96777, test loss: 74.74300\n",
      "epoch: 202, train loss: 210.36660, test loss: 73.27634\n",
      "epoch: 203, train loss: 211.83925, test loss: 76.78741\n",
      "epoch: 204, train loss: 208.20740, test loss: 72.69588\n",
      "epoch: 205, train loss: 197.72705, test loss: 71.09117\n",
      "epoch: 206, train loss: 196.54131, test loss: 70.04751\n",
      "epoch: 207, train loss: 197.94877, test loss: 71.40492\n",
      "epoch: 208, train loss: 191.73853, test loss: 68.39740\n",
      "epoch: 209, train loss: 196.55419, test loss: 71.80145\n",
      "epoch: 210, train loss: 203.54238, test loss: 71.89708\n",
      "epoch: 211, train loss: 205.91788, test loss: 74.59312\n",
      "epoch: 212, train loss: 198.73326, test loss: 70.58577\n",
      "epoch: 213, train loss: 196.13668, test loss: 70.22869\n",
      "epoch: 214, train loss: 197.61886, test loss: 71.87460\n",
      "epoch: 215, train loss: 206.41290, test loss: 72.74102\n",
      "epoch: 216, train loss: 189.67977, test loss: 67.70947\n",
      "epoch: 217, train loss: 182.67367, test loss: 65.61021\n",
      "epoch: 218, train loss: 180.08320, test loss: 63.46561\n",
      "epoch: 219, train loss: 192.24071, test loss: 73.06412\n",
      "epoch: 220, train loss: 229.84257, test loss: 78.06309\n",
      "epoch: 221, train loss: 243.07721, test loss: 86.78364\n",
      "epoch: 222, train loss: 209.66559, test loss: 73.49296\n",
      "epoch: 223, train loss: 193.92487, test loss: 68.41564\n",
      "epoch: 224, train loss: 184.46009, test loss: 66.57334\n",
      "epoch: 225, train loss: 180.05397, test loss: 63.59019\n",
      "epoch: 226, train loss: 187.91678, test loss: 71.36195\n",
      "epoch: 227, train loss: 219.44013, test loss: 74.87237\n",
      "epoch: 228, train loss: 246.28986, test loss: 88.78113\n",
      "epoch: 229, train loss: 221.33804, test loss: 78.86033\n",
      "epoch: 230, train loss: 202.00872, test loss: 70.28469\n",
      "epoch: 231, train loss: 212.72012, test loss: 76.83380\n",
      "epoch: 232, train loss: 189.06505, test loss: 67.14524\n",
      "epoch: 233, train loss: 184.18174, test loss: 67.01323\n",
      "epoch: 234, train loss: 176.27866, test loss: 62.85657\n",
      "epoch: 235, train loss: 179.16851, test loss: 67.43694\n",
      "epoch: 236, train loss: 188.19840, test loss: 65.32603\n",
      "epoch: 237, train loss: 231.10630, test loss: 87.02440\n",
      "epoch: 238, train loss: 237.34291, test loss: 80.87194\n",
      "epoch: 239, train loss: 232.04001, test loss: 81.73442\n",
      "epoch: 240, train loss: 211.03493, test loss: 75.56757\n",
      "epoch: 241, train loss: 192.46637, test loss: 69.25817\n",
      "epoch: 242, train loss: 173.05875, test loss: 61.69549\n",
      "epoch: 243, train loss: 173.78186, test loss: 65.09455\n",
      "epoch: 244, train loss: 199.22122, test loss: 68.68320\n",
      "epoch: 245, train loss: 278.02300, test loss: 99.62453\n",
      "epoch: 246, train loss: 197.06090, test loss: 70.89323\n",
      "epoch: 247, train loss: 188.94414, test loss: 67.04826\n",
      "epoch: 248, train loss: 190.24348, test loss: 69.45981\n",
      "epoch: 249, train loss: 185.60789, test loss: 65.29033\n",
      "epoch: 250, train loss: 213.57778, test loss: 78.79996\n",
      "epoch: 251, train loss: 192.07945, test loss: 66.72423\n",
      "epoch: 252, train loss: 230.23042, test loss: 84.81452\n",
      "epoch: 253, train loss: 205.09417, test loss: 71.46004\n",
      "epoch: 254, train loss: 193.50590, test loss: 69.61911\n",
      "epoch: 255, train loss: 169.50571, test loss: 61.24400\n",
      "epoch: 256, train loss: 164.45379, test loss: 59.00232\n",
      "epoch: 257, train loss: 162.33864, test loss: 58.26061\n",
      "epoch: 258, train loss: 160.70519, test loss: 57.22624\n",
      "epoch: 259, train loss: 159.19298, test loss: 57.23071\n",
      "epoch: 260, train loss: 159.05314, test loss: 56.65472\n",
      "epoch: 261, train loss: 167.32518, test loss: 64.05576\n",
      "epoch: 262, train loss: 210.41537, test loss: 72.37176\n",
      "epoch: 263, train loss: 298.73818, test loss: 104.99024\n",
      "epoch: 264, train loss: 219.09926, test loss: 81.32113\n",
      "epoch: 265, train loss: 205.19315, test loss: 71.29277\n",
      "epoch: 266, train loss: 196.65953, test loss: 71.58749\n",
      "epoch: 267, train loss: 174.90411, test loss: 62.23685\n",
      "epoch: 268, train loss: 173.17325, test loss: 66.49702\n",
      "epoch: 269, train loss: 186.13274, test loss: 64.82211\n",
      "epoch: 270, train loss: 269.14132, test loss: 98.29787\n",
      "epoch: 271, train loss: 203.59202, test loss: 72.93133\n",
      "epoch: 272, train loss: 173.30874, test loss: 62.86188\n",
      "epoch: 273, train loss: 171.66484, test loss: 64.63789\n",
      "epoch: 274, train loss: 179.54861, test loss: 62.97665\n",
      "epoch: 275, train loss: 228.98548, test loss: 86.70033\n",
      "epoch: 276, train loss: 201.89932, test loss: 69.54893\n",
      "epoch: 277, train loss: 223.77448, test loss: 82.76865\n",
      "epoch: 278, train loss: 195.13652, test loss: 68.75387\n",
      "epoch: 279, train loss: 170.07173, test loss: 62.86938\n",
      "epoch: 280, train loss: 158.21516, test loss: 56.74595\n",
      "epoch: 281, train loss: 159.98779, test loss: 60.96638\n",
      "epoch: 282, train loss: 178.63363, test loss: 62.99707\n",
      "epoch: 283, train loss: 263.40001, test loss: 97.27065\n",
      "epoch: 284, train loss: 188.63934, test loss: 66.03619\n",
      "epoch: 285, train loss: 179.54380, test loss: 68.51135\n",
      "epoch: 286, train loss: 168.89016, test loss: 59.61471\n",
      "epoch: 287, train loss: 184.17570, test loss: 73.96540\n",
      "epoch: 288, train loss: 213.36265, test loss: 73.30646\n",
      "epoch: 289, train loss: 255.05544, test loss: 93.05428\n",
      "epoch: 290, train loss: 212.92388, test loss: 74.95455\n",
      "epoch: 291, train loss: 178.16116, test loss: 64.74177\n",
      "epoch: 292, train loss: 162.28299, test loss: 59.78778\n",
      "epoch: 293, train loss: 155.60326, test loss: 56.09097\n",
      "epoch: 294, train loss: 160.21811, test loss: 62.28115\n",
      "epoch: 295, train loss: 185.02997, test loss: 65.06423\n",
      "epoch: 296, train loss: 277.05605, test loss: 100.03077\n",
      "epoch: 297, train loss: 170.30615, test loss: 62.67735\n",
      "epoch: 298, train loss: 155.65748, test loss: 55.99667\n",
      "epoch: 299, train loss: 157.16610, test loss: 58.99992\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(30, 50, 2, torch.float64)\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "train_loss, test_loss = [], []\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss.append(nn.loss(X_train, label_train).item())\n",
    "    test_loss.append(nn.loss(X_test, label_test).item())\n",
    "    nn.sgd_step(X_train, label_train, 1e-5)\n",
    "    \n",
    "    print('epoch: {}, train loss: {:.5f}, test loss: {:.5f}'.format(epoch, train_loss[-1], test_loss[-1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
